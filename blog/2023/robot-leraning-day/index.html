<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <p>Robot Learning Day 세미나 내용을 기록했습니다.</p> </blockquote> <h3 id="이경재-교수님-세미나">이경재 교수님 세미나</h3> <p><strong>Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</strong></p> <ul> <li>RLHF <ul> <li>Design Reward Function from Human Feedback</li> </ul> </li> <li>Preference: Starts with assuming. <ul> <li>Deterministic: underlined human’s return value</li> <li>Stochastic: Bradley-Terry model</li> </ul> </li> <li>Cognitive Load and Decision Fatigue <ul> <li>lead to incorrect, noisy labels</li> </ul> </li> </ul> <p>⇒ Using <strong>transitivity</strong>: 한 번의 feedback으로 많은 양의 preference label을 만들자.</p> <ul> <li> <table> <tbody> <tr> <td>Transitivity: Set of items $\mathcal{A}$, a &gt; b</td> <td>b &gt; c ⇒ a &gt; c</td> </tr> </tbody> </table> </li> <li>Linear Stochastic Transitivity <ul> <li>Bradley-Terry</li> </ul> </li> <li>Sequential Pairwise Comparison <ul> <li>Preference Labelling 이후에, <strong>하나의 component를 남겨놓음.</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">Increasing sequence</code>: j-th index 까지 increase하는 상황을 가정. (Average case.)</li> </ul> </li> </ul> </li> <li>Root Pairwise Comparison <ul> <li>지금까지 수행한 것 중 <strong>가장 선호하는 것을 Back-tracking.</strong> </li> <li>case가 훨씬 단순하며, augmentation의 양이 더욱 많음. (Best, Worst Case) ⇒ 새로운 trajectory data에 대해서 많은 양의 preference label이 추가됨. <ul> <li>보라색 line은 transitivity를 통해 얻어낼 수 있는 데이터를 뜻함.</li> </ul> </li> </ul> </li> </ul> <h4 id="is-it-always-beneficial">Is it always beneficial?</h4> <ul> <li>반드시 data dependency가 생기게 됨.</li> <li>Dependency Graph $G$ <ul> <li>error bound 유도가 이미 되어 있음.</li> <li>$\Delta G$: dependency graph <ul> <li>edge의 수가 해당 값인 degree를 의미함.</li> </ul> </li> <li>Every M roond 때에 dependency graph를 끊어줌.</li> </ul> </li> </ul> <h3 id="오윤선-교수님-세미나">오윤선 교수님 세미나</h3> <ul> <li>MAPF: Multi-Agent Path Finding <ul> <li>Conflict-Based Search: Optimality를 보장함.</li> <li>Priority-Based Search: 계획 속도 측면에서 효율적.</li> </ul> </li> <li>Robot Safety conflict <ul> <li>Cycle Conflict가 발생하게 됨. 서로 갇히게 됨.</li> </ul> </li> <li>Implicit language instruction이 제공되었을 때에 적절히 수행할 수 있는 연구를 수행 중임. <ul> <li><code class="language-plaintext highlighter-rouge">Get me something to tighten</code></li> <li> <code class="language-plaintext highlighter-rouge">Get me something to wear on</code> <ul> <li>Grasp pose를 retrieval</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">“대부분의 작업 계획은 상식을 기반으로 수행됨.”</code></li> <li>LLM + Feasibility를 확보하는 연구.</li> <li>Scene Graph를 Text-embedding으로 표현.</li> <li>VLM-based Task Planning <ul> <li><code class="language-plaintext highlighter-rouge">Tell me the order of which objects to pick up.</code></li> <li><strong>LaVIN</strong></li> </ul> </li> </ul> <h3 id="강민재-연구원-논문-소개">강민재 연구원 논문 소개</h3> <h4 id="object-rearrangement-planning-for-target-retrieval">Object Rearrangement Planning for Target Retrieval</h4> <ul> <li>Target 회수 문제 <ul> <li>Occlusion + Collision</li> <li>Unoccupied space is limited</li> </ul> </li> <li>기존의 가정: 모든 size를 알고 있음 + 모든 방향에서 잡을 수 있음. <ul> <li>shape+position을 추정 + grasping 자세가 제한됨</li> <li>NP-Hard ⇒ Sequential sub-problem <ul> <li>What is sub-problem?</li> <li>How can we solve this?</li> <li>Sequential sub-problem: able to solve?</li> </ul> </li> </ul> </li> <li>TSAD: Tree Search with Approaching Direction <ul> <li>Ref: GP3 paper; <strong>optimal theta</strong>를 얻게 됨.</li> <li>이러한 theta set을 <strong>Smallest Rearrangement Set</strong>으로 정의함. <ul> <li>이 집합이 공집합이 되면, collision 없이 물체를 꺼내올 수 있다는 것임.</li> </ul> </li> <li>Point Cloud-Based Simulation; <ul> <li>pointcloud를 움직이며 간접적으로 이해하려고 함. <ul> <li>Prehensile Decision Network</li> <li>Task-Specific State Reward Function</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="홍민의-연구원-논문-소개">홍민의 연구원 논문 소개</h3> <h4 id="diffused-task-agnostic-milestone-planner">Diffused Task-Agnostic Milestone Planner</h4> <p>Goal-conditioned RL</p> <ul> <li>Temporally-Sparese Milestone: sub-goal을 tracking 하게끔 함</li> <li>Predict milestones only once.</li> </ul> <h4 id="권오빈-연구원-논문-소개">권오빈 연구원 논문 소개</h4> <ul> <li>Grid-based Visual Navigation <ul> <li>RNR-Map: Renderable Neural Radiance Map</li> </ul> </li> </ul> <h3 id="기호건-연구원-논문-소개">기호건 연구원 논문 소개</h3> <h4 id="sdf-based-graph-convolutional-q-network">SDF-Based Graph Convolutional Q-Network</h4> <ul> <li>Find Action sequence <ul> <li>Sign-Distance Function을 기반으로 학습. <ul> <li>Fast Marching Method</li> <li>MDP의 state로 정의됨.</li> </ul> </li> </ul> </li> <li>SDFGCN <ul> <li>Scene Graph Generation <ul> <li>Init image / Final image를 기반으로, <strong>어떠한 방향으로 밀어낼지</strong>를 학습함.</li> <li>Complete Sub-graph를 생성함.</li> <li><strong>SDF representation이 나름 표현력이 뛰어남.</strong></li> </ul> </li> </ul> </li> </ul> <h3 id="오정우-연구원-논문-소개">오정우 연구원 논문 소개</h3> <h4 id="scan-socially-aware-navigation-using-mcts">SCAN: Socially-Aware Navigation Using MCTS</h4> <ul> <li>plan paths without considering future states or human-robot interaction <ul> <li>HRI must be considered!</li> </ul> </li> <li>MCTS로 multi-traj를 sampling <ul> <li>Simulation에서 Value-function 계산</li> <li>Real-world Deploy</li> <li>High-level planner를 만듦. <ol> <li>Cost-aware RRT-Star: Tree Search</li> <li>Value Prediction: <ol> <li>MCTS-sampled trajectory</li> <li>Local goal</li> <li>Global goal</li> </ol> </li> <li>Candidate Selection based on Value</li> </ol> </li> </ul> </li> <li>Evaluation <ul> <li>Task: Point Goal Navigation</li> <li>SANS: Socially-aware navigation score <ul> <li>CR: goal까지 온전하게 수행될 percentage</li> <li>SP: speed</li> <li>PE: Path efficiency</li> <li>SF: Safety score, LIDAR 값 중 최소값을 기준으로 safety-verification</li> <li>ST: Stability score; 이러한 Unsafe state에서 얼마나 빠르게 stable state로 빠져나왔는지.</li> </ul> </li> </ul> </li> </ul> </body></html>