<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <p>Arxiv. [<a href="https://arxiv.org/pdf/2312.09337.pdf" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://promptable-behaviors.github.io/" rel="external nofollow noopener" target="_blank">Project Page</a>]</p> <p>Minyoung Hwang<sup>1</sup>, Luca Weihs<sup>1</sup>, Chanwoo Park<sup>2</sup>, Kimin Lee<sup>3</sup>, Aniruddha Kembhavi<sup>1</sup>, Kiana Ehsani<sup>1</sup> <sup>1</sup>PRIOR @ Allen Institute for AI, <sup>2</sup>Massachusetts Institute of Technology, <sup>3</sup>Korea Advanced Institute of Science and Technology</p> <p>Dec. 14</p> </blockquote> <div align="center"> <img src="/assets/img/promptablebehaviors/overview.png" width="100%"> <p>Fig. 1: Overview of PromptableBehavior.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>promptable navigation behavior 연구를 선보였다.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존의 Embodied AI에서 hand-crafted reward design이 어려웠음. <ul> <li>Novel framework를 제시함: simplify the reward design process</li> </ul> </li> <li>3개의 interaction type을 통해 human preference를 추론했다. <ul> <li>(1) human demonstration, (2) preference feedback on trajectory comparison, (3) language instructions</li> </ul> </li> <li>ProcTHOR, RoboTHOR에서 실험을 수행함.</li> </ul> <h3 id="problem-formulation">Problem Formulation</h3> <ul> <li>가정하고 있는 점은 아래와 같음. <ul> <li>Human preference remains constant over time</li> <li>Each human preference is captured through a linear combination of multiple objectives in the environment</li> </ul> </li> <li>매 timestep t 마다 agent는 RGB observation $o_t$에 기반한 action $a_t$를 뱉어낸다. <ul> <li>action은 <code class="language-plaintext highlighter-rouge">[MoveAhead, RotateRight, RotateLeft, Done, LookUp, LookDown]</code>이 있음.</li> </ul> </li> <li>저자는 여기서 <code class="language-plaintext highlighter-rouge">agent’s (navigation) behavior</code>에 집중해, preference로 표현해주려고 함.</li> </ul> <h3 id="methodology-multi-objective-reinforcement-learning-morl">Methodology: Multi-Objective Reinforcement Learning (MORL)</h3> <div align="center"> <img src="/assets/img/promptablebehaviors/architecture.png" width="100%"> <p>Fig. 2: Architecture of PromptableBehavior.</p> </div> <h4 id="build-scene-representation">Build Scene Representation</h4> <ul> <li>multiple objective를 갖는 policy를 학습함. (conditioned on a <strong>reward weight vector</strong>) <ul> <li>(이는 <a href="https://arxiv.org/pdf/2211.09960.pdf" rel="external nofollow noopener" target="_blank">Ask4Help</a> 논문에서 영감을 얻었다고 함)</li> </ul> </li> <li> <strong>reward weight vector</strong>를 human preference에 일치하게 infer하도록 함. 이러한 <strong>reward weight</strong>에 대해서 추가적인 fine-tuning 없이 <strong>preference 조작</strong>이 가능함.</li> <li>저자가 제시하는 Promptable Behaviors는 2가지 단계로 이루어짐. <ul> <li>(1) Training a promptable multi-objective policy</li> <li>(2) Capturing the agent’s desired behavior through interactions.</li> </ul> </li> <li>Image encoding에는 CLIP 모델을 사용함.</li> <li>Reward weight encoder로는 feed-forward neural network(FFNN)을 활용해, $K \cdot 12$-dim latent codebook으로 표현함. <ul> <li>$r^{\mathbf{w}}=\mathbf{w^{\intercal}r}$, <ul> <li> <table> <tbody> <tr> <td>$\mathbf{w}$: randomly sampled from $K$-dim simplex $\Delta_K={\mathbf{w} \in \mathbb{R}^{K}_{+}</td> <td>~</td> <td> </td> <td>\mathbf{w}</td> <td> </td> <td>_{1}=1}$</td> </tr> </tbody> </table> </li> <li>기존의 연구들은 $\mathbf{w}$가 pre-defined 되어 있었지만, 저자는 이를 randomly exploration하겠다는 목적임. (그리고 이 reward weight vector $\mathbf{w}\in\Delta_K$인 user’s true preference로 표현된다고 가정한다.)</li> </ul> </li> </ul> </li> <li>Navigation policy: DD-PPO 모델로 수행함.</li> </ul> <h4 id="types-of-interaction-reasoning-through-interactions">Types of Interaction: reasoning through interactions</h4> <p>(1) Human Demonstration</p> <ul> <li>demonstrated action과 action distribution from policy $\pi$ 간의 log-likelihood loss로 계산함.</li> </ul> <p>(2) Trajectory Comparison</p> <ul> <li>일반적인 Bradley-Terry 구조로 수행함.</li> </ul> <p>(3) Language Instruction</p> <ul> <li>사용자가 제시하는 <strong>task description과 definition of objective</strong>를 토대로 ChatGPT가 <strong>optimal reward weight vector</strong> 값을 뱉어주도록 하였음. (In-Context Learning, Chain-of-Thought로 수행함.)</li> <li>Objective sets: <code class="language-plaintext highlighter-rouge">time efficiency, path efficiency, house exploration, safety</code> </li> </ul> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/promptablebehaviors/result.png" width="100%"> <p>Fig. 3: Results of PromptableBehavior.</p> </div> <h3 id="thoughts">Thoughts:</h3> <ul> <li>제가 생각하는 preference의 정의는 해당 논문에서 표현하듯, agent behavior에 기반한 것과 더 근접하다고 생각합니다.</li> <li>현재 저도 진행 중인 연구에 대해서 GPT-4V 모델이 human preference를 추론해주는 것만으로 contribution을 내세우기에는 어려움이 있을 것 같다고 여겨집니다. <ul> <li>저도 계속 구상해오던 점은, 해당 논문에서 수행한 것처럼 <strong>(1) 어떠한 reward policy를 학습하거나, (2) GPT가 preference weight를 뱉어주도록</strong> 하는 방향으로 수행하면 어떨까 라는 생각이 들었습니다. manipulator scene에 대해서도 충분히 arm motion에 대한 objective set을 정의해, 그에 따른 trajectory 결과도 보여줄 수 있을 것이라 생각합니다.</li> </ul> </li> <li>기존의 다른 preference-based RL 논문과 비교해 이 논문이 새로웠던 점은, K-dim reward weight에 대해 학습해주는 과정이라고 생각합니다. <ul> <li>이를 통해 기존에는 pre-defined and fixed w에 대해 수행된 것과 다르게, reward weight에 대해 exploration을 수행할 수 있었다고 생각합니다. (+ codebook representation)</li> </ul> </li> </ul> </body></html>