<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <table> <tbody> <tr> <td>CoRL 2023 (Oral). [<a href="https://hhoppe.com/poissonrecon.pdf" rel="external nofollow noopener" target="_blank">Paper</a>]</td> <td>[<a href="https://github.com/haonan16/Stow/" rel="external nofollow noopener" target="_blank">Code</a>]</td> </tr> </tbody> </table> <p>Haonan Chen, Yilong Niu<sup>*</sup>, Kaiwen Hong<sup>*</sup>, Shuijing Liu, Yixuan Wang, Yunzhu Li, Katherine Driggs-Campbell</p> <p>University of Illinois Urbana-Champaign</p> <p>(* indicate equal contribution)</p> <p>2023</p> </blockquote> <div align="center"> <img src="/assets/img/stow/stow-task-description.png" width="75%"> <p>Fig. 1: Task description about Stowing.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <p>요약: Stowing task에 대해서 one-shot demonstration으로 policy를 학습하자.</p> <ul> <li>Stowing: defined as relocating an object from a table to a cluttered shelf.</li> </ul> <blockquote> <p>We propose a framework that uses Graph Neural Networks (GNNs) to predict object interactions within the parameter space of behavior primitives.</p> </blockquote> <h4 id="previous-research">Previous Research</h4> <p>Stowing task는 아래 세 개의 측면에서 어려움이 있었음.</p> <ul> <li>Long-horizon nature: contact/friction에 대한 정의가 필요함.</li> <li>Multi-object interaction: struggle with adaptability.</li> <li>Variety of objects: need for expensive data collection and human labelling.</li> </ul> <h4 id="contribution">Contribution</h4> <ol> <li> <strong>minimal demonstration</strong>으로 학습할 수 있는 model-based imitation learning framework를 제시함.</li> <li>Stowing benchmark를 제시함. (comprehensive long-horizon task)</li> <li>저자가 제안한 모델에 대한 Effectiveness와 Generalization을 검증함.</li> </ol> <h4 id="approach">Approach</h4> <div align="center"> <img src="/assets/img/stow/stow-overview.png" width="100%"> <p>Fig. 2: Overview of STOW-GNN.</p> </div> <p>2개의 모듈로 나뉜다.</p> <ol> <li>GNN predicts system dynamics.</li> <li>A primitive-augmented trajectory optimization method achieves subgoals from a single demonstration.</li> </ol> <h5 id="learning-forward-dynamics-via-gnn">Learning Forward Dynamics via GNN</h5> <div align="center"> <img src="/assets/img/stow/stow-gnn.png" width="100%"> <p>Fig. 3: Description about GNN Network.</p> </div> <ul> <li>Graph Construction (Terminology) <ul> <li>Rigid-body system을 System state $s \in \mathcal{S}$ with $\mathcal{M}$ objects and $\mathcal{N}$ particles 로 표현한다. <ul> <li>graph state $s_{t}= \left(\mathcal{O}<em>{t}, \mathcal{E}</em>{t} \right)$</li> <li>grph’s vertices $\mathcal{O}_{t}$: <strong>object’s particles.</strong> <ul> <li>$o_{i,~t}=\left&lt; x_{i,~t},~c_{i,~t} \right&gt;$, particles’s position $x_{i,~t}$ and object’s attributes $c_{i,~t}$</li> </ul> </li> <li>grph’s edges $\mathcal{E}_{t}$: <strong>relations between particles.</strong> It is formed when the distance between two vertices is less than a specified threshold. <ul> <li>$\mathcal{E}_{t}$: 2개의 경우로 relation을 정의한다. <ol> <li>Intra-object relations: Between different particles within the same object or across different objects.</li> <li>Gripper-to-object relations: Between particles from the objects and the robot’s gripper. <ul> <li>$e_{k}=\left&lt; i_k, j_k, c_k \right&gt;$: $i_k, j_k$ are reciever and sender particles, respectively. $k$ is edge’s index, $c$ is object’s attribute</li> </ul> </li> </ol> </li> </ul> </li> </ul> </li> <li>Action을 $\mathcal{A}$로 표현한다. 이는 $\left&lt; \text{skill type, associated parameter} \right&gt;$ 를 포함하고 있다.</li> <li>$\mathcal{T}$: (dynamic object 각각에 대해) $\mathcal{M}$ 개의 rigid transformation을 의미한다.</li> <li>Dynamics function $\Phi: \mathcal{S\times A \rightarrow T}$</li> </ul> </li> <li>GNN을 사용하는 이유는 단순히 behavior primitive를 수행했을 때의 system’s state를 예측하기 위함이다.</li> </ul> <p>그렇게 모든 vertices와 edges는 MLP를 거쳐 latent vertex $h^{O}<em>{i}$, edge $h^{E}</em>{i,j}$ representation을 아래의 수식으로 표현할 수 있게 된다.</p> \[\begin{equation} h^{E}_{i,j} \leftarrow \rho^{E} \left( h^{E}_{i,j}, h^{O}_{i}, h^{O}_{j} \right),~~~ h^{O}_{i} \leftarrow \rho^{O} \left( h^{O}_{i}, \sum_{j} h^{E}_{i,j} \right) \end{equation}\] <p>여기서 $\rho^{E},~\rho^{O}$는 각각 edge, vertex의 message passing function (i.e., MLP)을 의미한다.</p> <p>그리고 각 물체의 rigid transformation는 decoder output의 mean으로 결정된다. (Fig. 2(b)를 참고하면 됨.)</p> <p>그리고 마지막으로 control action은 아래와 같이 정의된다. (gripper’s planned position and motion defined by particles.)</p> <ul> <li>$o_{i,t} = \left&lt; x_{i,t}, v_{i,t}, c_{i,t} \right&gt;$ <ul> <li>$x_{i,t}$: current position of gripper</li> <li>$v_{i,t}$: planned motion of the gripper</li> </ul> </li> </ul> <h5 id="control-with-the-learned-dynamics">Control with the Learned Dynamics</h5> <div align="center"> <img src="/assets/img/stow/stow-skill.png" width="100%"> <p>Fig. 4: Description about Skill selection.</p> </div> <p>Behavior primitive를 정의해 Operation Space Control로써 action이 수행된다. 저자는 총 3개의 Primitive를 정의한다.</p> <ol> <li>Sweeping: $y,h,d,\theta$로 정의됨. <ul> <li>$y$: starting offset in the shelf direction</li> <li>$h$: sweeping height</li> <li>$d$: sliding distance</li> <li>$\theta$: angle of gripper rotation</li> </ul> </li> <li>Pushing: $x,y,d$로 정의됨. <ul> <li>$(x,y)$: starting push(nudge) position</li> <li>$d$: distance of the push</li> </ul> </li> <li>Transporting: $y,h,d,\theta$로 정의되며, Sweeping과 동일함. 다른 점은, 물체를 쥐고 있다는 점이다.</li> </ol> <p>아래의 Objective loss를 최소화하는 action $a_{p}$를 찾는 것이 목적이다.</p> \[\begin{equation} a_{p} = \arg \min_{a_{p}} \mathcal{J} (s_{T}, g) \end{equation}\] <ul> <li>$a_{p}$: skill parameter</li> <li>$g$: Keyframes collected during demonstrations.</li> </ul> <p>요약: $\Phi$로 Forward prediction을 통해서 rigid body system의 future state를 예측하고, lowest cost를 갖는 skill parameter $a_{p}$를 찾는 것이다.</p> <h4 id="experiment-setup">Experiment Setup</h4> <div align="center"> <img src="/assets/img/stow/stow-exp-setup-all.png" width="100%"> <p>Fig. 5: Experimental Setup.</p> </div> <ul> <li>각 Action primitive 마다 300개의 episode dataset을 구축함. (각 skill에 따른 key pose도 포함됨.)</li> <li>shelf width로 randomization을 주었다. 그리고 shelf에 꽂힌 물체의 갯수/property/size/density에 대해서도 randomization을 수행했다.</li> <li>Kinova Gen3 Robot + OAK-D camera</li> <li>Evaluation metrics <ul> <li>Mean Squared Error (MSE)</li> <li>Earth Mover’s Distance (EMD)</li> <li>Chamfer distance (CD)</li> </ul> </li> </ul> <div align="center"> <img src="/assets/img/stow/stow-table-12.png" width="100%"> <p>Fig. 6: Results about STOW-GNN.</p> </div> <div align="center"> <img src="/assets/img/stow/stow-stillcut.png" width="100%"> <p>Fig. 7: Still-cut about STOW-GNN.</p> </div> <h3 id="thought">Thought</h3> <ul> <li>Shelf, Cluttered env에서 수행하는 pick-n-place를 stowing task라고 표현하는 것을 처음 알게 되었습니다.</li> <li>GNN 으로 rigid body dynamics를 예측하고 (정확히는 state(pose)를 예측하고), 이 예측값에 가장 근접하는 action primitive $a_{p}$를 찾는 연구입니다. 논문에서는 rigid body에 대해 모두 box shape의 point-cloud particle로 정의했는데, 다른 형태의 물체에 대해서는 실험을 수행하지 않은 점이 궁금합니다.</li> <li>해당 논문에서 보여준 transporting task는 꽤 괜찮은 task인 것 같습니다.</li> </ul> <h4 id="한계점">한계점</h4> <ul> <li>manual human labeling of ordered keyframes from demonstration</li> <li>used box-shaped point clouds to represent objects</li> </ul> </body></html>