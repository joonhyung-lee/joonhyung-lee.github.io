<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <table> <tbody> <tr> <td>NeurIPS 2023. [<a href="https://offline-rl-neurips.github.io/2021/pdf/24.pdf" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/ikostrikov/implicit_q_learning" rel="external nofollow noopener" target="_blank">Github<sup>1</sup></a> </td> <td> <a href="https://github.com/Manchery/iql-pytorch" rel="external nofollow noopener" target="_blank">Github<sup>2</sup></a>]</td> </tr> </tbody> </table> <p>Ilya Kostrikov, Ashvin Nair &amp; Sergey Levine Department of Electrical Engineering and Computer Science University of California, Berkeley</p> <p>12 Oct 2021</p> </blockquote> <h2 id="한-문장-요약">한 문장 요약</h2> <p>요약: State value function을 random variable로 정의하여, policy improvement를 implicit하게 근사해보자. 구체적으로는 Expectiles of the state value function을 추정해보자.</p> <blockquote> <p>Offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization</p> <p>Keyword: <strong>Reinforcement Learning</strong>, <strong>Offline RL</strong>, <strong>Quantile Regression</strong></p> </blockquote> <h3 id="introduction">Introduction</h3> </body></html>