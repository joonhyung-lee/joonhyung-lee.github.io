<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <p>ICLR. [<a href="https://openreview.net/forum?id=8HCARN2hhw" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://statler-lm.github.io/" rel="external nofollow noopener" target="_blank">Project Page</a>]</p> <p>Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca Monaci and Christian Wolf <sup>1</sup> <sup>1</sup>Naver Labs Europe, Meylan, France</p> <p>Sep. 29</p> </blockquote> <div align="center"> <img src="/assets/img/mole/overview.png" width="50%"> <p>Fig. 1: Overview of MOLE.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>Navigability를 정의하고, 이 latent spatial representation을 학습하자.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li>Instead of learning to reconstruct, they cast the robotic perception task as a navigation task by a blind auxiliary agent generating a learning signal for the main agent.</li> </ul> <div align="center"> <img src="/assets/img/mole/concept.png" width="75%"> <p>Fig. 2: Concept of MOLE.</p> </div> <div align="center"> <img src="/assets/img/mole/architecture.png" width="100%"> <p>Fig. 3: Architecture of MOLE.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>They propose learning a latent spatial representation (i.e., Navigability). <ul> <li>This approach differs from traditional methods that rely on explicit scene reconstruction. Instead, it relies on a learned latent spatial representation of the environment for navigation.</li> </ul> </li> <li>They define representation $r_t$ and optimize it based on its amount of information. This representation is refined by a blind auxiliary agent, which operates without direct visual observations, thereby testing and refining its utility for navigation.</li> <li>The author describes the difference between the two methods based on <strong>Behavior Cloning</strong> and <strong>Navigability</strong>. <ul> <li> <strong>BC</strong> directly learns the main target policy from expert trajectories, approximating the desired optimal policy. <strong>Navigability</strong>, on the other hand, focuses on learning a representation that optimizes navigational skills (i.e., actions) like detecting navigable space and avoiding obstacles, rather than reconstructing the scene in detail.</li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I thought that the proposed method seems like a teacher-student network. The main policy (teacher) provides a latent spatial representation (teaching material) that the blind auxiliary agent (student) uses to learn navigational actions.</li> <li>The auxiliary agent’s performance in navigating using this representation gives feedback to improve the main agent’s ability to create effective latent representations. This method offers a more flexible and potentially robust way for robots to navigate diverse environments, especially where creating or relying on detailed maps is impractical or impossible.</li> <li>I think that It’s a notable step forward in the development of autonomous systems that can adapt to a wide range of real-world conditions.</li> </ul> </body></html>