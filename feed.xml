<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://joonhyung-lee.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://joonhyung-lee.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-28T14:50:18+00:00</updated><id>https://joonhyung-lee.github.io//feed.xml</id><title type="html">Joel Lee</title><subtitle>Robotics Research Master&apos;s Student </subtitle><entry><title type="html">[paper-review] Reactive Base Control for On-The-Move Mobile Manipulation in Dynamic Environments</title><link href="https://joonhyung-lee.github.io//blog/2024/rbc/" rel="alternate" type="text/html" title="[paper-review] Reactive Base Control for On-The-Move Mobile Manipulation in Dynamic Environments"/><published>2024-01-28T00:00:00+00:00</published><updated>2024-01-28T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/rbc</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/rbc/"><![CDATA[<blockquote> <p>RA-L. [<a href="https://arxiv.org/pdf/2309.09393.pdf">Paper</a>] [<a href="https://benburgesslimerick.github.io/MotM-BaseControl/">Project Page</a>]</p> <p>Ben Burgess-Limerick<sup>1, 2</sup>, Jesse Haviland<sup>1, 2</sup>, Chris Lehnert<sup>1</sup>, Peter Corke<sup>1</sup> <sup>1</sup>Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia <sup>2</sup> CSIRO Data61, Brisbane, Australia</p> <p>Mar. 03</p> </blockquote> <div align="center"> <img src="/assets/img/rbc/rbc-overview.png" width="50%"/> <p>Fig. 1: Overview of Reactive Base Control.</p> </div> <h3 id="summary">Summary</h3> <ul> <li>This paper presents a reactive-based control method for mobile manipulation in dynamic environments, which significantly reduces the task time and improves the performance of the robot while avoiding static and dynamic obstacles.</li> <li>This approach contrasts with traditional methods where the base and manipulator are controlled separately, often resulting in increased task time due to the sequential completion of navigation and manipulation tasks​.</li> </ul> <div align="center"> <img src="/assets/img/rbc/rbc-staa-orientation.png" width="75%"/> <p>Fig. 2: Inclusion of orientation cost in STAA*.</p> </div> <div align="center"> <img src="/assets/img/rbc/rbc-pathptr.png" width="50%"/> <p>Fig. 3: Bezier path evaluation with PathRTR.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>The paper introduces a reactive base control system for mobile manipulation in dynamic environments, enabling tasks to be performed on-the-move and significantly reducing total task time. This work marks the first real-world demonstration of reactive manipulation with both static and dynamic obstacles, showcasing the method’s practical applicability and robustness. ​ <ul> <li>1) They used STAA* algorithm that plangs a collision-free global path by searching through a visibility graph, and then compuites the intersection of the global path and the borader of a local planning region to develop an intermediate goal. The most important addtion to STAA* is the inclusion of an orientation to the goal state. They consider orientation which enables poses to be achieved that smoothly connects the immediate target with the next goal.</li> <li>2) In A* graph search module, they employed PathRTR heuristic. And they combines it with an additional heuristic based on a Bezier curve. It encourages the exploration of states that can be connected to the goal through smooth curves. They said that a value of 25% results in curves that work well in experimental setup.</li> <li>3) Base Placement was optimized by simple cost functions by the weighted sum of two components: <ul> <li> \[C_i = C_{i,C} + 1.05 \cdot C_{i,N}\] <ul> <li>\(C_{i,C}\) is the estimated cost from <strong>robot to candidate.</strong></li> <li>\(C_{i,N}\) is the estimated cost from <strong>the candidate to the next target.</strong></li> </ul> </li> <li>The path cost for each candidate is evaluated using the PathRTR metric.</li> </ul> </li> <li>4) Arm Obstacle Avoidance: They leveraged Quadratic Program to calculate joint veloicited for a given desired end-effector and base velocity. The controller allows for slack in the achieved end-effector velocity.</li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I was already aware of a previous paper by this author, Manipulation On-The-Move, and it seems to be a follow-up to both this paper and an algorithm called STAA*.</li> <li>I think the contribution of this paper is to add a couple of heuristics to the existing algorithm, and I would have to start with the previous paper; the baseline controller implemented in this paper is all based on the previous paper as a reference. <ul> <li>STAA* / MotM / PathRTR</li> </ul> </li> <li>After reading the paper and watching the demo video, I was quite impressed with the holistic approach to motion planning for mobile-manipulator systems; so accurate and high performance.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Manipulation"/><category term="Mobile Manipulation"/><category term="Manipulation on-the-Move (MotM)"/><category term="Navigation"/><category term="RA-L"/><category term="2024"/><summary type="html"><![CDATA[paper review about Reactive Base Control]]></summary></entry><entry><title type="html">[paper-review] Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction</title><link href="https://joonhyung-lee.github.io//blog/2024/mole/" rel="alternate" type="text/html" title="[paper-review] Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction"/><published>2024-01-20T00:00:00+00:00</published><updated>2024-01-20T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/mole</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/mole/"><![CDATA[<blockquote> <p>ICLR. [<a href="https://openreview.net/forum?id=8HCARN2hhw">Paper</a>]</p> <p>Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca Monaci and Christian Wolf <sup>1</sup> <sup>1</sup>Naver Labs Europe, Meylan, France</p> <p>Sep. 29</p> </blockquote> <div align="center"> <img src="/assets/img/mole/overview.png" width="50%"/> <p>Fig. 1: Overview of MOLE.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>Navigability를 정의하고, 이 latent spatial representation을 학습하자.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li>Instead of learning to reconstruct, they cast the robotic perception task as a navigation task by a blind auxiliary agent generating a learning signal for the main agent.</li> </ul> <div align="center"> <img src="/assets/img/mole/concept.png" width="75%"/> <p>Fig. 2: Concept of MOLE.</p> </div> <div align="center"> <img src="/assets/img/mole/architecture.png" width="100%"/> <p>Fig. 3: Architecture of MOLE.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>They propose learning a latent spatial representation (i.e., Navigability). <ul> <li>This approach differs from traditional methods that rely on explicit scene reconstruction. Instead, it relies on a learned latent spatial representation of the environment for navigation.</li> </ul> </li> <li>They define representation \(r_t\) and optimize it based on its amount of information. This representation is refined by a blind auxiliary agent, which operates without direct visual observations, thereby testing and refining its utility for navigation.</li> <li>The author describes the difference between the two methods based on <strong>Behavior Cloning</strong> and <strong>Navigability</strong>. <ul> <li><strong>BC</strong> directly learns the main target policy from expert trajectories, approximating the desired optimal policy. <strong>Navigability</strong>, on the other hand, focuses on learning a representation that optimizes navigational skills (i.e., actions) like detecting navigable space and avoiding obstacles, rather than reconstructing the scene in detail.</li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I thought that the proposed method seems like a teacher-student network. The main policy (teacher) provides a latent spatial representation (teaching material) that the blind auxiliary agent (student) uses to learn navigational actions.</li> <li>The auxiliary agent’s performance in navigating using this representation gives feedback to improve the main agent’s ability to create effective latent representations. This method offers a more flexible and potentially robust way for robots to navigate diverse environments, especially where creating or relying on detailed maps is impractical or impossible.</li> <li>I think that It’s a notable step forward in the development of autonomous systems that can adapt to a wide range of real-world conditions.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Navigation"/><category term="Navigation"/><category term="GRU"/><category term="ICLR"/><category term="2024"/><summary type="html"><![CDATA[paper review about Mole]]></summary></entry><entry><title type="html">[paper-review] Statler: State-Maintaining Language Models for Embodied Reasoning</title><link href="https://joonhyung-lee.github.io//blog/2024/statler/" rel="alternate" type="text/html" title="[paper-review] Statler: State-Maintaining Language Models for Embodied Reasoning"/><published>2024-01-07T00:00:00+00:00</published><updated>2024-01-07T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/statler</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/statler/"><![CDATA[<blockquote> <p>Arxiv. [<a href="https://arxiv.org/pdf/2306.17840.pdf">Paper</a>] [<a href="https://statler-lm.github.io/">Project Page</a>]</p> <p>Takuma Yoneda<sup>*1</sup>, Jiading Fang<sup>*1</sup>, Peng Li<sup>*2</sup>, Huanyu Zhang<sup>*3</sup>, Tianchong Jiang<sup>3</sup>, Shengjie Lin<sup>1</sup>, Ben Picker<sup>3</sup>, David Yunis<sup>1</sup> Hongyuan Mei<sup>1</sup>, Matthew R. Walter<sup>1</sup> <sup>1</sup>TTI-Chicago, <sup>2</sup>Fudan University, <sup>3</sup>University of Chicago, *Equal Contribution</p> <p>Dec. 4</p> </blockquote> <div align="center"> <img src="/assets/img/statler/statler_teaser.png" width="100%"/> <p>Fig. 1: Overview of Statler.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>World State를 갱신해가며 LLM Reasoning을 수행하자.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존 LLM 방식은 자신이 뱉어준 Action과 Observation에 기인해 reasoning을 수행해왔음. <ul> <li>Traditional LLMs in robotics generate actions based only on prior actions and observations, lacking <strong>an explicit</strong> world state.</li> </ul> </li> <li>2개의 prompted LLM을 통해 world-state를 <strong>explicit</strong>하게 maintaining 하겠다는 의도. <blockquote> <p>Statler utilizes a pair of prompted LLMs: instructed by a few demonstrations, <strong>the world-state reader</strong> takes as input the user query, reads the estimated world state, and generates an executable action (e.g, a code snippet); instructed by another set of demonstrations, <strong>the world-state writer</strong> updates the world state estimate based on the action.</p> </blockquote> </li> <li>이를 통해 Long-Horizon LLM Interaction에서 기억을 소실하거나, misleading reasoning을 방지해준다고 함.</li> </ul> <h3 id="motivation">Motivation</h3> <div align="center"> <img src="/assets/img/statler/statler-motivation.png" width="100%"/> <p>Fig. 2: Motivation of Statler.</p> </div> <ul> <li>야바위 게임처럼, 컵 안의 물체는 계속해서 움직이게 되는데, 우리가 명시적으로 공의 움직임을 관찰하지는 못하지만, internal-representation에 의해 공이 어떠한 컵에 들어있는지 알 수 있다.</li> <li>이것에서 영감을 얻어, LLM이 관측하지 못하는 world-state에 대해 maintaining하는 방향으로 연구를 수행함.</li> </ul> <h3 id="methodology">Methodology</h3> <div align="center"> <div style="display: flex; justify-content: center; align-items: center;"> <div style="flex: 50%;"> <img src="/assets/img/statler/statler-world-reader.png" style="width: 100%;"/> <p>Fig. 3: World State Reader.</p> </div> <div style="flex: 50%;"> <img src="/assets/img/statler/statler-world-writer.png" style="width: 100%;"/> <p>Fig. 4: World State Writer.</p> </div> </div> </div> <ul> <li>World-State-Reader <ul> <li>reader는 현재 state를 고려한 action을 취해주는 LLM 역할.</li> <li>여기서 state가 update 되어야 하는 부분도 고려해서 답변을 만듦.</li> </ul> </li> <li>World-State-Writer <ul> <li>앞선 reader가 추론해낸 state에 기반해, state를 upadte하고, external memory에 저장함. (이것이 곧 current-state가 됨.)</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/statler/statler-example.png" width="50%"/> <p>Fig. 5: Example scenario about Statler.</p> </div> <h3 id="thoughts">Thoughts:</h3> <ul> <li>저자가 언급한 야바위 문제로 motivation을 얘기하느데, 이 점이 꽤나 재밌게 느껴졌습니다.</li> <li>지금 GPT-4V로 연구를 수행하며 느낀 점이, long-horizon interaction을 수행할 때에 기억을 소실한다는 것이었습니다.</li> <li>저자는 GPT-4 모델로 <strong>state-reader, state-writer</strong> 두 개로 역할을 나누어 이러한 기억 소실을 방지해내고자 했습니다. <ul> <li>appendix도 읽어보았으나, prompt에 대해 novel한 부분을 찾지는 못했습니다. world-state를 template에 맞게 뱉어주고, 이에 기반해 world-state를 업데이트 시켰다고 하는데, 단순히 이것 만으로 기억 소실을 개선시켰다는 점이 신기합니다.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/LLM"/><category term="LLM"/><category term="Reasoning"/><category term="Arxiv"/><category term="2024"/><summary type="html"><![CDATA[paper review about Statler]]></summary></entry><entry><title type="html">[paper-review] Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</title><link href="https://joonhyung-lee.github.io//blog/2023/promptablebehaviors/" rel="alternate" type="text/html" title="[paper-review] Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences"/><published>2023-12-24T00:00:00+00:00</published><updated>2023-12-24T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/promptablebehaviors</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/promptablebehaviors/"><![CDATA[<blockquote> <p>Arxiv. [<a href="https://arxiv.org/pdf/2312.09337.pdf">Paper</a>] [<a href="https://promptable-behaviors.github.io/">Project Page</a>]</p> <p>Minyoung Hwang<sup>1</sup>, Luca Weihs<sup>1</sup>, Chanwoo Park<sup>2</sup>, Kimin Lee<sup>3</sup>, Aniruddha Kembhavi<sup>1</sup>, Kiana Ehsani<sup>1</sup> <sup>1</sup>PRIOR @ Allen Institute for AI, <sup>2</sup>Massachusetts Institute of Technology, <sup>3</sup>Korea Advanced Institute of Science and Technology</p> <p>Dec. 14</p> </blockquote> <div align="center"> <img src="/assets/img/promptablebehaviors/overview.png" width="100%"/> <p>Fig. 1: Overview of PromptableBehavior.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>promptable navigation behavior 연구를 선보였다.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존의 Embodied AI에서 hand-crafted reward design이 어려웠음. <ul> <li>Novel framework를 제시함: simplify the reward design process</li> </ul> </li> <li>3개의 interaction type을 통해 human preference를 추론했다. <ul> <li>(1) human demonstration, (2) preference feedback on trajectory comparison, (3) language instructions</li> </ul> </li> <li>ProcTHOR, RoboTHOR에서 실험을 수행함.</li> </ul> <h3 id="problem-formulation">Problem Formulation</h3> <ul> <li>가정하고 있는 점은 아래와 같음. <ul> <li>Human preference remains constant over time</li> <li>Each human preference is captured through a linear combination of multiple objectives in the environment</li> </ul> </li> <li>매 timestep t 마다 agent는 RGB observation \(o_t\)에 기반한 action \(a_t\)를 뱉어낸다. <ul> <li>action은 <code class="language-plaintext highlighter-rouge">[MoveAhead, RotateRight, RotateLeft, Done, LookUp, LookDown]</code>이 있음.</li> </ul> </li> <li>저자는 여기서 <code class="language-plaintext highlighter-rouge">agent’s (navigation) behavior</code>에 집중해, preference로 표현해주려고 함.</li> </ul> <h3 id="methodology-multi-objective-reinforcement-learning-morl">Methodology: Multi-Objective Reinforcement Learning (MORL)</h3> <div align="center"> <img src="/assets/img/promptablebehaviors/architecture.png" width="100%"/> <p>Fig. 2: Architecture of PromptableBehavior.</p> </div> <h4 id="build-scene-representation">Build Scene Representation</h4> <ul> <li>multiple objective를 갖는 policy를 학습함. (conditioned on a <strong>reward weight vector</strong>) <ul> <li>(이는 <a href="https://arxiv.org/pdf/2211.09960.pdf">Ask4Help</a> 논문에서 영감을 얻었다고 함)</li> </ul> </li> <li><strong>reward weight vector</strong>를 human preference에 일치하게 infer하도록 함. 이러한 <strong>reward weight</strong>에 대해서 추가적인 fine-tuning 없이 <strong>preference 조작</strong>이 가능함.</li> <li>저자가 제시하는 Promptable Behaviors는 2가지 단계로 이루어짐. <ul> <li>(1) Training a promptable multi-objective policy</li> <li>(2) Capturing the agent’s desired behavior through interactions.</li> </ul> </li> <li>Image encoding에는 CLIP 모델을 사용함.</li> <li>Reward weight encoder로는 feed-forward neural network(FFNN)을 활용해, \(K \cdot 12\)-dim latent codebook으로 표현함. <ul> <li>\(r^{\mathbf{w}}=\mathbf{w^{\intercal}r}\), <ul> <li> <table> <tbody> <tr> <td>\(\mathbf{w}\): randomly sampled from \(K\)-dim simplex $$\Delta_K={\mathbf{w} \in \mathbb{R}^{K}_{+}</td> <td>~</td> <td> </td> <td>\mathbf{w}</td> <td> </td> <td>_{1}=1}$$</td> </tr> </tbody> </table> </li> <li>기존의 연구들은 \(\mathbf{w}\)가 pre-defined 되어 있었지만, 저자는 이를 randomly exploration하겠다는 목적임. (그리고 이 reward weight vector \(\mathbf{w}\in\Delta_K\)인 user’s true preference로 표현된다고 가정한다.)</li> </ul> </li> </ul> </li> <li>Navigation policy: DD-PPO 모델로 수행함.</li> </ul> <h4 id="types-of-interaction-reasoning-through-interactions">Types of Interaction: reasoning through interactions</h4> <p>(1) Human Demonstration</p> <ul> <li>demonstrated action과 action distribution from policy \(\pi\) 간의 log-likelihood loss로 계산함.</li> </ul> <p>(2) Trajectory Comparison</p> <ul> <li>일반적인 Bradley-Terry 구조로 수행함.</li> </ul> <p>(3) Language Instruction</p> <ul> <li>사용자가 제시하는 <strong>task description과 definition of objective</strong>를 토대로 ChatGPT가 <strong>optimal reward weight vector</strong> 값을 뱉어주도록 하였음. (In-Context Learning, Chain-of-Thought로 수행함.)</li> <li>Objective sets: <code class="language-plaintext highlighter-rouge">time efficiency, path efficiency, house exploration, safety</code></li> </ul> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/promptablebehaviors/result.png" width="100%"/> <p>Fig. 3: Results of PromptableBehavior.</p> </div> <h3 id="thoughts">Thoughts:</h3> <ul> <li>제가 생각하는 preference의 정의는 해당 논문에서 표현하듯, agent behavior에 기반한 것과 더 근접하다고 생각합니다.</li> <li>현재 저도 진행 중인 연구에 대해서 GPT-4V 모델이 human preference를 추론해주는 것만으로 contribution을 내세우기에는 어려움이 있을 것 같다고 여겨집니다. <ul> <li>저도 계속 구상해오던 점은, 해당 논문에서 수행한 것처럼 <strong>(1) 어떠한 reward policy를 학습하거나, (2) GPT가 preference weight를 뱉어주도록</strong> 하는 방향으로 수행하면 어떨까 라는 생각이 들었습니다. manipulator scene에 대해서도 충분히 arm motion에 대한 objective set을 정의해, 그에 따른 trajectory 결과도 보여줄 수 있을 것이라 생각합니다.</li> </ul> </li> <li>기존의 다른 preference-based RL 논문과 비교해 이 논문이 새로웠던 점은, K-dim reward weight에 대해 학습해주는 과정이라고 생각합니다. <ul> <li>이를 통해 기존에는 pre-defined and fixed w에 대해 수행된 것과 다르게, reward weight에 대해 exploration을 수행할 수 있었다고 생각합니다. (+ codebook representation)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/PBL"/><category term="PBL"/><category term="LLM"/><category term="Reasoning"/><category term="NeurIPS-W"/><category term="2023"/><summary type="html"><![CDATA[paper review about Dream2Real]]></summary></entry><entry><title type="html">[seminar] Robot Learning Day</title><link href="https://joonhyung-lee.github.io//blog/2023/robot-leraning-day/" rel="alternate" type="text/html" title="[seminar] Robot Learning Day"/><published>2023-12-22T00:00:00+00:00</published><updated>2023-12-22T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/robot-leraning-day</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/robot-leraning-day/"><![CDATA[<blockquote> <p>Robot Learning Day 세미나 내용을 기록했습니다.</p> </blockquote> <h3 id="이경재-교수님-세미나">이경재 교수님 세미나</h3> <p><strong>Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</strong></p> <ul> <li>RLHF <ul> <li>Design Reward Function from Human Feedback</li> </ul> </li> <li>Preference: Starts with assuming. <ul> <li>Deterministic: underlined human’s return value</li> <li>Stochastic: Bradley-Terry model</li> </ul> </li> <li>Cognitive Load and Decision Fatigue <ul> <li>lead to incorrect, noisy labels</li> </ul> </li> </ul> <p>⇒ Using <strong>transitivity</strong>: 한 번의 feedback으로 많은 양의 preference label을 만들자.</p> <ul> <li> <table> <tbody> <tr> <td>Transitivity: Set of items \(\mathcal{A}\), a &gt; b</td> <td>b &gt; c ⇒ a &gt; c</td> </tr> </tbody> </table> </li> <li>Linear Stochastic Transitivity <ul> <li>Bradley-Terry</li> </ul> </li> <li>Sequential Pairwise Comparison <ul> <li>Preference Labelling 이후에, <strong>하나의 component를 남겨놓음.</strong> <ul> <li><code class="language-plaintext highlighter-rouge">Increasing sequence</code>: j-th index 까지 increase하는 상황을 가정. (Average case.)</li> </ul> </li> </ul> </li> <li>Root Pairwise Comparison <ul> <li>지금까지 수행한 것 중 <strong>가장 선호하는 것을 Back-tracking.</strong></li> <li>case가 훨씬 단순하며, augmentation의 양이 더욱 많음. (Best, Worst Case) ⇒ 새로운 trajectory data에 대해서 많은 양의 preference label이 추가됨. <ul> <li>보라색 line은 transitivity를 통해 얻어낼 수 있는 데이터를 뜻함.</li> </ul> </li> </ul> </li> </ul> <h4 id="is-it-always-beneficial">Is it always beneficial?</h4> <ul> <li>반드시 data dependency가 생기게 됨.</li> <li>Dependency Graph \(G\) <ul> <li>error bound 유도가 이미 되어 있음.</li> <li>\(\Delta G\): dependency graph <ul> <li>edge의 수가 해당 값인 degree를 의미함.</li> </ul> </li> <li>Every M roond 때에 dependency graph를 끊어줌.</li> </ul> </li> </ul> <h3 id="오윤선-교수님-세미나">오윤선 교수님 세미나</h3> <ul> <li>MAPF: Multi-Agent Path Finding <ul> <li>Conflict-Based Search: Optimality를 보장함.</li> <li>Priority-Based Search: 계획 속도 측면에서 효율적.</li> </ul> </li> <li>Robot Safety conflict <ul> <li>Cycle Conflict가 발생하게 됨. 서로 갇히게 됨.</li> </ul> </li> <li>Implicit language instruction이 제공되었을 때에 적절히 수행할 수 있는 연구를 수행 중임. <ul> <li><code class="language-plaintext highlighter-rouge">Get me something to tighten</code></li> <li><code class="language-plaintext highlighter-rouge">Get me something to wear on</code> <ul> <li>Grasp pose를 retrieval</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">“대부분의 작업 계획은 상식을 기반으로 수행됨.”</code></li> <li>LLM + Feasibility를 확보하는 연구.</li> <li>Scene Graph를 Text-embedding으로 표현.</li> <li>VLM-based Task Planning <ul> <li><code class="language-plaintext highlighter-rouge">Tell me the order of which objects to pick up.</code></li> <li><strong>LaVIN</strong></li> </ul> </li> </ul> <h3 id="강민재-연구원-논문-소개">강민재 연구원 논문 소개</h3> <h4 id="object-rearrangement-planning-for-target-retrieval">Object Rearrangement Planning for Target Retrieval</h4> <ul> <li>Target 회수 문제 <ul> <li>Occlusion + Collision</li> <li>Unoccupied space is limited</li> </ul> </li> <li>기존의 가정: 모든 size를 알고 있음 + 모든 방향에서 잡을 수 있음. <ul> <li>shape+position을 추정 + grasping 자세가 제한됨</li> <li>NP-Hard ⇒ Sequential sub-problem <ul> <li>What is sub-problem?</li> <li>How can we solve this?</li> <li>Sequential sub-problem: able to solve?</li> </ul> </li> </ul> </li> <li>TSAD: Tree Search with Approaching Direction <ul> <li>Ref: GP3 paper; <strong>optimal theta</strong>를 얻게 됨.</li> <li>이러한 theta set을 <strong>Smallest Rearrangement Set</strong>으로 정의함. <ul> <li>이 집합이 공집합이 되면, collision 없이 물체를 꺼내올 수 있다는 것임.</li> </ul> </li> <li>Point Cloud-Based Simulation; <ul> <li>pointcloud를 움직이며 간접적으로 이해하려고 함. <ul> <li>Prehensile Decision Network</li> <li>Task-Specific State Reward Function</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="홍민의-연구원-논문-소개">홍민의 연구원 논문 소개</h3> <h4 id="diffused-task-agnostic-milestone-planner">Diffused Task-Agnostic Milestone Planner</h4> <p>Goal-conditioned RL</p> <ul> <li>Temporally-Sparese Milestone: sub-goal을 tracking 하게끔 함</li> <li>Predict milestones only once.</li> </ul> <h4 id="권오빈-연구원-논문-소개">권오빈 연구원 논문 소개</h4> <ul> <li>Grid-based Visual Navigation <ul> <li>RNR-Map: Renderable Neural Radiance Map</li> </ul> </li> </ul> <h3 id="기호건-연구원-논문-소개">기호건 연구원 논문 소개</h3> <h4 id="sdf-based-graph-convolutional-q-network">SDF-Based Graph Convolutional Q-Network</h4> <ul> <li>Find Action sequence <ul> <li>Sign-Distance Function을 기반으로 학습. <ul> <li>Fast Marching Method</li> <li>MDP의 state로 정의됨.</li> </ul> </li> </ul> </li> <li>SDFGCN <ul> <li>Scene Graph Generation <ul> <li>Init image / Final image를 기반으로, <strong>어떠한 방향으로 밀어낼지</strong>를 학습함.</li> <li>Complete Sub-graph를 생성함.</li> <li><strong>SDF representation이 나름 표현력이 뛰어남.</strong></li> </ul> </li> </ul> </li> </ul> <h3 id="오정우-연구원-논문-소개">오정우 연구원 논문 소개</h3> <h4 id="scan-socially-aware-navigation-using-mcts">SCAN: Socially-Aware Navigation Using MCTS</h4> <ul> <li>plan paths without considering future states or human-robot interaction <ul> <li>HRI must be considered!</li> </ul> </li> <li>MCTS로 multi-traj를 sampling <ul> <li>Simulation에서 Value-function 계산</li> <li>Real-world Deploy</li> <li>High-level planner를 만듦. <ol> <li>Cost-aware RRT-Star: Tree Search</li> <li>Value Prediction: <ol> <li>MCTS-sampled trajectory</li> <li>Local goal</li> <li>Global goal</li> </ol> </li> <li>Candidate Selection based on Value</li> </ol> </li> </ul> </li> <li>Evaluation <ul> <li>Task: Point Goal Navigation</li> <li>SANS: Socially-aware navigation score <ul> <li>CR: goal까지 온전하게 수행될 percentage</li> <li>SP: speed</li> <li>PE: Path efficiency</li> <li>SF: Safety score, LIDAR 값 중 최소값을 기준으로 safety-verification</li> <li>ST: Stability score; 이러한 Unsafe state에서 얼마나 빠르게 stable state로 빠져나왔는지.</li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="seminar"/><category term="Robotics and Learning"/><summary type="html"><![CDATA[seminar summary about Robot Learning Day]]></summary></entry><entry><title type="html">[miscellaneous] docker usage</title><link href="https://joonhyung-lee.github.io//blog/2023/docker/" rel="alternate" type="text/html" title="[miscellaneous] docker usage"/><published>2023-12-13T00:00:00+00:00</published><updated>2023-12-13T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/docker</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/docker/"><![CDATA[<blockquote> <p>기타 유용한 <code class="language-plaintext highlighter-rouge">Docker</code> 명령어에 대해 작성합니다.</p> </blockquote> <h3 id="make-dockerfile">Make Dockerfile</h3> <ul> <li>Get docker image <ul> <li><code class="language-plaintext highlighter-rouge">docker pull nvidia/cuda:11.6.1-devel-ubuntu20.04</code></li> </ul> </li> <li>Dockerfile <ul> <li><strong>반드시 파일의 제목은 <code class="language-plaintext highlighter-rouge">Dockerfile</code> 이어야 한다. (확장자는 없음.)</strong></li> </ul> </li> </ul> <h4 id="example-dockerfile">Example <strong>Dockerfile</strong></h4> <div class="language-docker highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 기존 이미지를 기반으로 사용</span>
<span class="c"># Use nvidia/cuda version matches your server</span>
<span class="k">FROM</span><span class="s"> nvidia/cuda:11.6.1-cudnn8-devel-ubuntu20.04</span>

<span class="c"># 필요한 패키지 설치</span>
<span class="c"># Install ubuntu apt packages. Do not remove default packages.</span>
<span class="k">RUN </span>apt-get update
<span class="c"># opencv-python error</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nv">DEBIAN_FRONTEND</span><span class="o">=</span>noninteractive apt-get <span class="nb">install</span><span class="se">\
</span>    libgl1<span class="se">\
</span>    libgl1-mesa-glx <span class="se">\ </span>
    libglib2.0-0 -y &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="nv">DEBIAN_FRONTEND</span><span class="o">=</span>noninteractive apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    wget <span class="se">\
</span>    apt-utils <span class="se">\
</span>    build-essential <span class="se">\
</span>    ca-certificates <span class="se">\
</span>    curl <span class="se">\
</span>    git <span class="se">\
</span>    htop <span class="se">\
</span>    <span class="nb">sudo</span> <span class="se">\
</span>    vim <span class="se">\
</span>    python3-dev <span class="se">\
</span>    python3-pip <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>

<span class="c"># Miniconda 설치</span>
<span class="k">RUN </span>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh <span class="nt">-O</span> /miniconda.sh
<span class="k">RUN </span>bash /miniconda.sh <span class="nt">-b</span> <span class="nt">-p</span> /miniconda
<span class="k">ENV</span><span class="s"> PATH="/miniconda/bin:${PATH}"</span>

<span class="c"># Conda를 이용해 Python3 설치 및 Python 패키지 설치</span>
<span class="k">RUN </span>conda <span class="nb">install</span> <span class="nt">-y</span> <span class="nv">python</span><span class="o">=</span>3.9 <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip3 <span class="nt">--no-cache-dir</span> <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="se">\
</span>    pip <span class="se">\
</span>    setuptools <span class="se">\
</span>    ipython <span class="se">\
</span>    ipdb <span class="se">\
</span>    matplotlib <span class="se">\
</span>    pandas <span class="se">\
</span>    scipy <span class="se">\
</span>    torch <span class="se">\
</span>    jupyter <span class="se">\
</span>    torchvision <span class="se">\
</span>    torchtext <span class="se">\
</span>    torchsummary <span class="se">\
</span>    slacker <span class="se">\
</span>    tqdm

<span class="c"># # 사용자 설정</span>
<span class="c"># ARG UNAME</span>
<span class="c"># ARG UID</span>
<span class="c"># ARG GID</span>

<span class="c"># # Ensure that the group and user are created successfully</span>
<span class="c"># RUN if [ -z "$UNAME" ] || [ -z "$UID" ] || [ -z "$GID" ]; then echo "UNAME, UID, and GID arguments are required" &amp;&amp; exit 1; fi &amp;&amp; \</span>
<span class="c">#     addgroup --gid ${GID} ${UNAME} &amp;&amp; \</span>
<span class="c">#     useradd -m -u ${UID} -g ${GID} -s /bin/bash ${UNAME} &amp;&amp; \</span>
<span class="c">#     adduser ${UNAME} sudo</span>

<span class="c"># USER ${UNAME}</span>
<span class="c"># WORKDIR /home/${UNAME}</span>

<span class="k">CMD</span><span class="s"> [ "/bin/bash" ]</span>
</code></pre></div></div> <details> <summary>Build Log</summary> <div> <div class="language-docker highlighter-rouge"><div class="highlight"><pre class="highlight"><code>joonhyung@devbox:~/dockers/joonh_cu116$ docker build -t joonh_cu116 .
[+] Building 260.0s (11/11) FINISHED                                                                                                                                                    
 =&gt; [internal] load .dockerignore                                                                                                                                                  0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                                    0.0s
 =&gt; [internal] load build definition from Dockerfile                                                                                                                               0.0s
 =&gt; =&gt; transferring dockerfile: 1.72kB                                                                                                                                             0.0s
 =&gt; [internal] load metadata for docker.io/nvidia/cuda:11.6.1-cudnn8-devel-ubuntu20.04                                                                                             0.0s
 =&gt; [1/7] FROM docker.io/nvidia/cuda:11.6.1-cudnn8-devel-ubuntu20.04                                                                                                               0.0s
 =&gt; CACHED [2/7] RUN apt-get update                                                                                                                                                0.0s
 =&gt; [3/7] RUN apt-get update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install    libgl1    libgl1-mesa-glx     libglib2.0-0 -y &amp;&amp;     rm -rf /var/lib/apt/lists/*                19.2s
 =&gt; [4/7] RUN apt-get update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y     wget     apt-utils     build-essential     ca-certificates     curl     git     htop     s  24.1s
 =&gt; [5/7] RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /miniconda.sh                                                                          3.0s
 =&gt; [6/7] RUN bash /miniconda.sh -b -p /miniconda                                                                                                                                  6.4s
 =&gt; [7/7] RUN conda install -y python=3.9 &amp;&amp;     pip3 --no-cache-dir install --upgrade     pip     setuptools     ipython     ipdb     matplotlib     pandas     scipy     torc  177.4s
 =&gt; exporting to image                                                                                                                                                            29.8s
 =&gt; =&gt; exporting layers                                                                                                                                                           29.8s
 =&gt; =&gt; writing image sha256:48e3f2be75424e71be48c75d5472c90a9bf5ac9c94586bd723127d5bed67714b                                                                                       0.0s 
 =&gt; =&gt; naming to docker.io/library/joonh_cu116
</code></pre></div> </div> </div> </details> <h4 id="build-docker-image">Build docker image</h4> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> <span class="o">[</span>이미지 이름:이미지 버전] <span class="o">[</span>Dockerfile의 경로]
docker build <span class="nt">-t</span> joonh_cu116 <span class="nb">.</span>
docker build <span class="nt">--build-arg</span> <span class="nv">UNAME</span><span class="o">=</span>joonhyung-lee <span class="nt">--build-arg</span> <span class="nv">UID</span><span class="o">=</span>1001 <span class="nt">--build-arg</span> <span class="nv">GID</span><span class="o">=</span>1001 <span class="nt">-t</span> joonh_cu116 <span class="nb">.</span>
</code></pre></div></div> <h4 id="get-built-docker-images">Get built docker images</h4> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker images
<span class="o">&gt;&gt;&gt;</span> joonh_cu116    latest    7c1bae5ab933   33 seconds ago   9.56GB
</code></pre></div></div> <h4 id="start-docker-container">Start Docker Container</h4> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-v</span> <span class="o">[</span>로컬_경로]:[컨테이너_경로] <span class="nt">-it</span> <span class="nt">--gpus</span> all <span class="o">[</span>이미지_이름]:[태그] /bin/bash
docker run <span class="nt">-v</span> /home/joonhyung/python/:/home/root/python <span class="nt">-it</span> <span class="nt">--gpus</span> all joonh_cu116:latest /bin/bash
</code></pre></div></div> <h2 id="commit-and-push-docker-container">Commit and Push Docker Container</h2> <h3 id="get-information-about-container">get information about container</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>joonhyung@devbox:~/dockers/joonh_cu116<span class="nv">$ </span>docker ps
CONTAINER ID   IMAGE                              COMMAND                  CREATED             STATUS             PORTS                                       NAMES
79b602e29923   c2e91d98d3ed                       <span class="s2">"/opt/nvidia/nvidia_…"</span>   About an hour ago   Up About an hour                                               magical_neumann
</code></pre></div></div> <h3 id="commit">Commit</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker commit CONTAINER IMAGE_NAME
joonhyung@devbox:~/dockers/joonh_cu116<span class="nv">$ </span>docker commit 79b602e29923 joonh_cu116:v00
sha256:f50aabee25697dc96bc80f76fd231c74d1fbfe77cb47698a4ec89e0b84c5ba81
</code></pre></div></div> <ul> <li>docker images로 image가 적절하게 생성되었는지 확인 가능.</li> </ul> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  joonhyung@devbox:~/dockers/joonh_cu116<span class="nv">$ </span>docker images
  REPOSITORY                                                  TAG                               IMAGE ID       CREATED          SIZE
  joonh_cu116                                                 v00                               f50aabee2569   14 seconds ago   31GB
</code></pre></div></div> <h3 id="build-dockerfile">Build <strong><code class="language-plaintext highlighter-rouge">Dockerfile</code></strong></h3> <ul> <li>Add lines <ul> <li> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FROM joonh_cu116:v00
<span class="c"># Install ubuntu apt packages.</span>
RUN <span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> &lt;ubuntu-packages&gt;
</code></pre></div> </div> </li> </ul> </li> <li>Build New version <ul> <li> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> joonh_cu116:v01 <span class="nb">.</span>
</code></pre></div> </div> </li> </ul> </li> </ul> <h3 id="login-docker-server">Login Docker server</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  joonhyung@devbox:~/dockers/joonh_cu116_v00<span class="nv">$ </span>docker login <span class="nt">-u</span> joonhyunglee
  Password: 
  WARNING! Your password will be stored unencrypted <span class="k">in</span> /home/joonhyung/.docker/config.json.
  Configure a credential helper to remove this warning. See
  https://docs.docker.com/engine/reference/commandline/login/#credentials-store

  Login Succeeded
</code></pre></div></div> <h3 id="push-on-docker">push on Docker</h3> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>joonhyung@devbox:~/dockers/joonh_cu116_v00<span class="nv">$ </span>docker push joonhyunglee/joonh_cu116:v01
</code></pre></div></div>]]></content><author><name></name></author><category term="miscellaneous"/><category term="Docker"/><summary type="html"><![CDATA[Docker Usage]]></summary></entry><entry><title type="html">[paper-review] Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models</title><link href="https://joonhyung-lee.github.io//blog/2023/dream2real/" rel="alternate" type="text/html" title="[paper-review] Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models"/><published>2023-12-09T00:00:00+00:00</published><updated>2023-12-09T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/dream2real</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/dream2real/"><![CDATA[<blockquote> <p>NeurIPS-W 2023 Oral. [<a href="https://arxiv.org/pdf/2312.04533.pdf">Paper</a>] [<a href="https://www.robot-learning.uk/dream2real">Project Page</a>]</p> <p>Ivan Kapelyukh<sup>1, 2</sup> , Yifei Ren<sup>1</sup> , Ignacio Alzugaray<sup>2</sup> , Edward Johns<sup>1</sup> The Robot Learning Lab at Imperial College London<sup>1</sup>. The Dyson Robotics Lab at Imperial College London<sup>2</sup></p> <p>Dec. 07</p> </blockquote> <div align="center"> <img src="/assets/img/dream2real/overview.png" width="100%"/> <p>Fig. 1: Overview of Dream-to-Real.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>NeRF로 TSDF mesh로 만들어 physics check를 하고, Textual+Visual reasoning으로 best pose를 얻어내자.</li> </ul> <h3 id="contribution">Contribution</h3> <ul> <li>기존의 방법론인 DALEE-E-Bot 과 비교하면 아래의 개선점이 있음. <ul> <li>DALL-E-Bot can only generate 2D images, so is unsuitable for 6-DoF tasks.</li> <li>It is easier to imagine new images by reconfiguring NeRFs, than by learning to generate images.</li> </ul> </li> <li>Zero-shot Manner로 Robotic rearrangement task를 수행할 수 있다. <ul> <li>just <strong>dreaming</strong> and <strong>evaluating</strong></li> </ul> </li> <li>Robot builds an object-centric NeRF of a scene.</li> <li>Numerous reconfigurations of the scene are rendered as 2D images.</li> <li>CLIP evaluates these according to the user instruction.</li> <li>The best is then physically created using pick-and-place.</li> </ul> <h3 id="methodology">Methodology:</h3> <div align="center"> <img src="/assets/img/dream2real/framework.png" width="100%"/> <p>Fig. 2: Framework of Dream-to-Real.</p> </div> <h4 id="build-scene-representation">Build Scene Representation</h4> <ul> <li>RGB-D 카메라로 workspace를 반구 형태로 capture: TSDF Format <ul> <li>Workspace, Target object: NeRF (Instant-NGP)</li> </ul> </li> <li>[SAM+XMem] Segmentatoin VLMs + BLIP-2 <ul> <li>해당 모델로 workspace에 대해 semented workspace + target object (background/foreground)를 얻을 수 있다.</li> <li>각 view의 object에 대해 captioning을 BLIP-2 model로 수행함.</li> </ul> </li> </ul> <h4 id="imagine-best-pose">Imagine Best Pose</h4> <ul> <li>Workspace에 해당하는 Grid로 샘플링 + Orientation도 discrete set 내에서 샘플링 -&gt; 앞서 생성된 TSDF mesh를 기반으로 physics(collision) check <ul> <li><code class="language-plaintext highlighter-rouge">To build the physics models, we combine depth images from across views to create a separate foreground and background Truncated Signed Distance Function (TSDF) [49], which we find achieves more accurate geometry than extracting a mesh from Instant-NGP.</code></li> <li><code class="language-plaintext highlighter-rouge">We move (virtually) the movable object’s physics model to each of the sampled poses in turn and check for physical validity, i.e. the object must not be in collision with the scene or unsupported in free space.</code></li> </ul> </li> <li>GPT-4 Input: [movable object, relevant objects, the goal caption and the normalising caption] <ul> <li>movable object: 직접 움직이게 될 물체를 의미합니다.</li> <li>relevant object: task의 수행 여부를 판단하기 위해 관측해야 하는 object or region을 의미합니다.</li> <li>goal caption: description of the desired final state after the instruction has been fulfilled.</li> <li>normalising caption: spatial relationship을 잘 포착하게끔 해주는 목적이라고 합니다. (<code class="language-plaintext highlighter-rouge">description of the scene that remains neutral to the pose of the object being moved.</code>)</li> </ul> </li> </ul> <h4 id="execute-pick-and-place">Execute Pick-and-Place</h4> <p>Execute Robot action in real-world</p> <h3 id="experiments">Experiments</h3> <div align="center"> <img src="/assets/img/dream2real/environment.png" width="100%"/> <p>Fig. 3: Enviroments of Dream2Real.</p> </div> <ul> <li>Setup; 3개의 서로 다른 real-world scene + 10개의 rearrangement task</li> <li>Evaluation metric; Measure task success using success regions.</li> <li>H/W; Franka + D435i</li> </ul> <h4 id="baselines">Baselines</h4> <ul> <li>DALL-E-Bot</li> <li>D2R-One-View, Only use one view: which uses only the first camera view throughout the whole pipeline (including object captioning), avoiding the need for data collection.</li> <li>D2R-Distract: do not use GPT-4 (distractor)</li> <li>Physics-Only: do not use CLIP to evaluate poses</li> <li>D2R-No-Norm: no normalizing captions</li> <li>D2R-Vis-Prior</li> <li>D2R-NoSmooth: ablates spatial smoothing</li> </ul> <h4 id="results">Results</h4> <div align="center"> <img src="/assets/img/dream2real/table1-shopping.png" width="100%"/> <p>Fig. 4: Table about Shopping Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/qualitative-result-shopping.png" width="100%"/> <p>Fig. 4.1: Qualitative results from Shopping Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/table2-pool-ball.png" width="75%"/> <p>Fig. 5: Table about Pool Ball Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/qualitative-result-pool-ball.png" width="100%"/> <p>Fig. 5.1: Qualitative results from Pool-Ball Env.</p> </div> <div align="center"> <img src="/assets/img/dream2real/success-rate-plot.png" width="100%"/> <p>Fig. 6: Success Rate plot for baselines.</p> </div> <h5 id="visualize-heat-map">Visualize heat map</h5> <div align="center"> <img src="/assets/img/dream2real/visualize-renderings.png" width="100%"/> <p>Fig. 7: Visualize renderings from Dream2Real.</p> </div> <h3 id="conclusion--limitation">Conclusion &amp; Limitation</h3> <ul> <li>Conclusion <ul> <li>저자는 zero-shot manner로 2D VLMs로 3D object rearrangement task를 수행할 수 있는 점을 큰 contribution으로 삼습니다.</li> <li>추후 연구로는 iterative하게 score를 refine하는 방식도 될 수 있을 것이며, multistage-task에 대해서도 수행할 수 있을 것입니다.</li> </ul> </li> <li>Limitation <ul> <li>하나의 scene을 NeRF로 reconstruct 하는 것에 3~5분 정도 소요된다고 합니다.</li> <li>orientation을 discrete set에서 샘플하여, orientation이 더욱 복잡한 task에는 적용하기 어렵다고 합니다. (sampling high-resolution -&gt; high computation cost)</li> <li>CLIP에서 bag-of words behaviour의 현상이 일어납니다. 이는 Text에서 단어의 순서가 중요한 경우 입니다. <ul> <li>e.g. <code class="language-plaintext highlighter-rouge">“a fork to the left of a knife” often places the knife to the left of the fork instead."</code></li> </ul> </li> </ul> </li> </ul> <h3 id="thoughts">Thoughts:</h3> <ul> <li>spots 논문의 컨셉과 유사한 것 같아 읽었습니다. 해당 논문에서는 NeRF로 전체 workspace와 target object를 복원하고, target object의 orientation도 고려해가며 physics check를 합니다.</li> <li>다만 NeRF 기반의 방식이라 하나의 씬에 3~5분 정도 걸린다는 한계가 있다고 저자가 밝힙니다. real-to-sim 측면을 조금 강조하려면 해당 연구에서 한 방식대로 3d mesh를 만들어 내는 것이 좋을 것 같습니다.</li> <li>그리고 해당 논문에서는 score heatmap을 정의해 physics-check + similarity check를 통해 best-pose 하나만을 찾습니다. CLIP으로 이미지와 desired goal-pose(caption) 간의 similarity로 best pose를 찾습니다.</li> <li>여기서 goal-caption은 object pose에 대한 내용만을 이루며, 저희가 spots에서 수행했던 semantic reasoning과는 다르다고 느꼈습니다.</li> <li>spots 논문에 이어서, physics check를 여러 robot action에 대해서 수행하는 방향으로 확장하면 재밌을 것 같습니다.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/cv"/><category term="real-to-sim"/><category term="LLM"/><category term="Reasoning"/><category term="VLM"/><category term="NeurIPS-W"/><category term="2023"/><summary type="html"><![CDATA[paper review about Dream2Real]]></summary></entry><entry><title type="html">[paper-review] Motion Planning Networks</title><link href="https://joonhyung-lee.github.io//blog/2023/mpnet/" rel="alternate" type="text/html" title="[paper-review] Motion Planning Networks"/><published>2023-11-26T00:00:00+00:00</published><updated>2023-11-26T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/mpnet</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/mpnet/"><![CDATA[<blockquote> <p>ICRA 2019. [<a href="https://arxiv.org/pdf/1806.05767.pdf">Paper</a>] [<a href="https://github.com/ahq1993/MPNet/tree/master">Github</a>]</p> <p>Ahmed H. Qureshi, Anthony Simeonov, Mayur J. Bency and Michael C. Yip <sup>1</sup> University of California San Diego, La Jolla, CA 92093 USA<sup>1</sup></p> <p>Feb. 24</p> </blockquote> <div align="center"> <img src="/assets/img/mpnet/mpnet-overview.png" width="75%"/> <p>Fig. 1: Introduction figure about MPNet paper.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <p>Learnable한 Collision-free Motion Planning Network를 제시한다.</p> <h3 id="contribution">Contribution</h3> <div align="center"> <img src="/assets/img/mpnet/mpnet-architecture.png" width="100%"/> <p>Fig. 2: Overall architecture about MPNet.</p> </div> <ul> <li>Offline: 1) Encoder Netowork, 2) Planning Network</li> <li>Online: Neural Planner</li> </ul> <h3 id="terminology">Terminology:</h3> <ul> <li>\(Q\): ordered list of length \(N \in \mathbb{N}\)</li> <li>\(\{q_{i}=Q(i)\}_{i\in N}\): mapping from \(i\in \mathbb{N}\) to the \(i\)-th element of \(Q\).</li> <li>\(X \subset \mathbb{R}^{d}\): state space, where \(d\in \mathbb{N}\) is the dimensionality of the state space. <ul> <li>\(d_{w}\in \mathbb{N}\): workspace dimensionality</li> </ul> </li> <li>\(X_{\text{obs}} \subset X\): obstacle state space</li> <li>\(X_{\text{free}} \subset X\textbackslash X_{\text{obs}}\): obstacle state space <ul> <li>Initial state \(x_{\text{init}} \in X_{\text{free}}\), goal region be \(X_{\text{goal}} \subset X_{\text{free}}\)</li> </ul> </li> </ul> <p>목적은 solution path \(\tau\)가 entirely obstacle-free space \(X_{\text{free}}\)에 존재하도록 하는 것이다.</p> <h3 id="methodology">Methodology:</h3> <p>(A) offline training of the neural models, and (B) online path generation; 2개의 모듈로 구성되어 있다.</p> <div align="center"> <img src="/assets/img/mpnet/mpnet-alg-enc.png" width="75%"/> <p>Fig. 3: Algorithm about MPNet (Offline).</p> </div> <ol> <li>Offline Training <ul> <li>Obstacle point cloud(=scene)을 latent space에 embedding 시키고 (Encoder Network), obstacle encoding \(Z\)로부터 Planning Network로 \(\hat{x}_{t+1}\)을 추정한다.</li> <li><strong>Enet: Encoder Network</strong> <ul> <li>Contractive Autoencoder를 사용했다. <ul> <li>obstacle pointcloud \(X_{\text{obs}}\)를 \(Z\in \mathbb{R}^m\)에 embedding 시킴.</li> </ul> </li> </ul> </li> </ul> </li> </ol> \[\begin{equation} L_{\text{AE}}(\theta^{e},\theta^{d})=\frac{1}{N_{\text{obs}}} \sum_{x\in D_{\text{obs}}} || x-\hat{x} ||^2 + \lambda \sum_{ij} (\theta^{e}_{ij})^{2} \end{equation}\] <ul> <li>\(\theta^{e}\): parameters of encoder</li> <li> <p>\(\theta^{d}\): parameters of decoder</p> </li> <li><strong>Pnet: Planning Network</strong> <ul> <li> \[\hat{x}_{t+1}=\text{Pnet}((x_{t},x_{T},\mathbf{Z});\mathbf{\theta})\] </li> <li>Expert data로 학습을 진행함. (MSE Loss)</li> </ul> </li> </ul> \[\begin{equation} L_{\text{Pnet}}(\theta)=\frac{1}{N_{p}} \sum_{j}^{\hat{N}}\sum_{i=0}^{T-1} || \hat{x}_{j,i+1}-x_{j,i+1} ||^2 \end{equation}\] <div align="center"> <img src="/assets/img/mpnet/mpnet-alg-plan.png" width="75%"/> <p>Fig. 4: Algorithm about MPNet (Neural Planner).</p> </div> <div align="center"> <img src="/assets/img/mpnet/mpnet-alg-replan.png" width="75%"/> <p>Fig. 5: Algorithm about MPNet (Replanning).</p> </div> <ol> <li>Online Path Planning <ul> <li>Offline에서 학습한 모델을 기반으로 (collision-free) motion planning을 수행하기 위해 Bidirectional path generation heuristic을 제시함. <ul> <li>Pnet: stochaticity를 추가하기 위해 일정 확률로 dropout 을 수행함. (\(p: [0.1]\in \mathbb{R}\))</li> <li>Lazy States Contraction (LSC): connect_tree 함수와 동일함.</li> <li>Steering: 일반적인 planner의 steer 함수와 동일함.</li> <li>isFeasible: 일반적인 planner의 feasibility 함수와 동일함.</li> <li>Neural Planner: 저자가 말하는 Bidirectional path generation. <ul> <li>\(\tau^{a}, \tau^{b}\): 각각 init state로부터 시작하는 path, goal state로부터 시작하는 path를 의미함.</li> <li>우선 \(\tau^{a}\)부터 범위를 확장해가며, planning을 수행함. 여기서 not connectable 한다면 \(\tau^{b}\)부터 범위를 확장해감: <strong>swap function</strong></li> </ul> </li> </ul> </li> </ul> </li> </ol> <h3 id="implementation--experiments">Implementation &amp; Experiments</h3> <ul> <li>110개의 서로 다른 workspace: simple 2D, rigid-body, complex 2D/3D <ul> <li>100개의 workspace, 각 4000개의 trajectory로 학습을 수행함. 여기서 사용된 trajectory는 RRT\(^*\)로부터 취득함.</li> </ul> </li> <li>Enet: three linerar layers and an output layer, Parametric ReLU를 사용함.</li> <li> <p>Pnet: 9-layers and 12-layers DNN, PReLU를 사용함. 모든 은닉층에 dropout도 포함되어 있음.</p> </li> <li>Baeslines: MPNet+Neural-Replanning / MPNet+Hybrid-Replanning / Informed-RRT\(^*\) / BIT\(^*\)</li> <li>Evaluation metric: Time [s] / Accuracy [%]</li> </ul> <div align="center"> <img src="/assets/img/mpnet/mpnet-results.png" width="100%"/> <p>Fig. 6: Results about MPNet.</p> </div> <h3 id="conclusion">Conclusion:</h3> <p>MPNet은 (1) obstacle geometry에 관계없이 motion planning을 잘 하며, (2) exec. time이 평균적으로 1초 이내에 수행되었으며, (3) unseen obstacle location에서도 잘 동작했으며, (4) completeness를 보장한다.</p> <h3 id="thoughts">Thoughts:</h3> <ul> <li>Encoder에 Scene pointcloud가 들어가며, <strong>Contractive Autoencoder</strong>로 obstacle에 대해 embedding 시킨다는 점에서 제가 진행하고자 하는 연구와 유사한 점을 느꼈습니다. <ul> <li>다만 단순한 Linear layer(PReLU) 만으로 pointcloud 데이터를 학습했다는 것이 신기합니다. 해당 시기에 PointNet/FoldingNet/DGCNN 등 pointcloud based architecture가 여러 있었는데도 불구하고 이렇게 수행했다는 것이 신기합니다. 제가 알기로는 rigid motion에 대해 invariant 해야할텐데, 이를 단순 linear layer 만으로 어떻게 잘 학습했는지가 궁금합니다.</li> </ul> </li> <li>앞서 학습된 Autoencoder를 사용해, expert trajectory로 planner를 학습합니다. <ul> <li>알고리즘상으로는 RRT-Star와 큰 차이는 없으며, Bidirectional하게 수행된다는 것이 차이점이라고 생각됩니다.</li> </ul> </li> <li>논문에서는 Obstacle pointcloud가 입력으로 들어간다고 밝히지만, 정확히 어떠한 형식으로 사용되는지는 설명이 조금 부족하다고 느껴집니다.</li> <li>CAE 모델에 대해서 간단하게 MNIST로 코드를 수행해봤었는데, Point cloud data로도 학습이 잘 수행되는지 확인해봐야 할 것 같습니다.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/robotics"/><category term="Motion Planning"/><category term="Auto Encoder"/><category term="ICRA"/><category term="2019"/><summary type="html"><![CDATA[paper review about MPNet]]></summary></entry><entry><title type="html">[paper-review] Dynamic CNN for Learning on Point Clouds</title><link href="https://joonhyung-lee.github.io//blog/2023/dgcnn/" rel="alternate" type="text/html" title="[paper-review] Dynamic CNN for Learning on Point Clouds"/><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/dgcnn</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/dgcnn/"><![CDATA[<blockquote> <p>ACM 2019. [<a href="https://arxiv.org/abs/1801.07829">Paper</a>] [<a href="https://github.com/WangYueFt/dgcnn">Github</a>]</p> <p>Yue Wang<sup>1</sup>, Yongbin Sun<sup>1</sup>, Ziwei Liu<sup>2</sup>, Sanjay E. Sarma<sup>1</sup>, Michael M. Bronstein<sup>3</sup>, Justin M. Solomon<sup>1</sup> Salah Rifai<sup>1</sup>, Pascal Vincent<sup>1</sup>, Xavier Muller<sup>1</sup>, Xavier Glorot<sup>1</sup>, Yoshua Bengio<sup>1</sup></p> <p>Jun. 11</p> </blockquote> <h3 id="한-문장-요약">한 문장 요약</h3> <p>Point Cloud의 Feature를 Graph CNN 구조로 추출해보자.</p> <h3 id="contribution">Contribution</h3> <ul> <li><strong>EdgeConv</strong>: <ul> <li>local geometric structure를 얻어냄.</li> <li><code class="language-plaintext highlighter-rouge">edge feature</code>를 만들어내며, 이는 point와 its neighbor의 relationship을 나타낸다. 그리고 당연하게도 이는 permutation invariant 하다. <ul> <li>기존의 PointNet++ 에서는 local feature에 대해 포착하지 못하는 한계점을 극복하고자 한다.</li> </ul> </li> </ul> </li> <li>layer를 거쳐가며, dynamic하게 업데이트 되는 grpah에서도 잘 작동한다.</li> <li>쉽게 다른 방법론에 붙일 수 있다.</li> </ul> <h3 id="approach">Approach:</h3> <h5 id="edge-convolution">Edge Convolution</h5> <p>The output of EdgeConv at the i-th vertex is given by</p> \[\begin{equation} \mathbf{x}'_{i}=\square_{k:(i,j)\in \mathcal{E}} h_{\mathcal{\Theta}}(x_i,x_j) \end{equation}\] <ul> <li>\(F\)-dimensional point cloud with \(n\) points; \(\mathbf{X}=\{\mathbf{x}_{1},\cdots,\mathbf{x}_{n}\}\)</li> <li>Graph \(\mathcal{G=(V,E)}\) representing local point cloud structure, where \(\mathcal{V}=\{1,\cdots,n\}\) and \(\mathcal{E}\subseteq \mathcal{V\times V}\) are the \(\text{vertices}\) and \(\text{edges}\), respectively. <ul> <li>Construct \(\mathcal{G}\) as the k-nearnest neighbor (k-NN) graph of \(\mathbf{X}\) in \(\mathbb{R}^{F}\).</li> <li>Graph includes self-loop, \(\text{edge~features}\) as \(\mathbf{e}_{ij}=h_{\Theta}(x_i,x_j)\), where \(h_{\Theta}:\mathbb{R}^{F} \times \mathbb{R}^{F} \rightarrow \mathbb{R}^{F'}\)</li> </ul> </li> </ul> <h5 id="dynamic-graph-update">Dynamic graph update</h5> <h5 id="properties">Properties</h5> <h5 id="comparison-to-existing-methods">Comparison to existing methods</h5> <h3 id="conclusion">Conclusion:</h3> <h3 id="thoughts">Thoughts:</h3>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/cv"/><category term="Point Cloud"/><category term="Auto Encoder"/><category term="ACM"/><category term="2019"/><summary type="html"><![CDATA[paper review about DGCNN]]></summary></entry><entry><title type="html">[seminar] Making Robots See and Manipulate</title><link href="https://joonhyung-lee.github.io//blog/2023/im-2/" rel="alternate" type="text/html" title="[seminar] Making Robots See and Manipulate"/><published>2023-11-21T00:00:00+00:00</published><updated>2023-11-21T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2023/im%5E2</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2023/im-2/"><![CDATA[<blockquote> <p>김범준 교수님의 세미나 <strong><code class="language-plaintext highlighter-rouge">Making Robots See and Manipulate</code></strong> 내용을 기록했습니다.</p> </blockquote> <ul> <li>Continuous motion level reasoning: Feasibility check가 필수적이며, 이는 computation expensive. <ul> <li>Idea: Learn to <strong>guide Planning</strong> \(\rightarrow \red{\text{MCTS+RL}}\) <ul> <li>Tree search + Value function, policy to guide the search.</li> <li>어떠한 물체를 어떻게, 어디로 옮겨야 하는지 geometric reasoning에 기반한 planning을 수행함.</li> </ul> </li> </ul> </li> </ul> <p>그러나 real world에서 로봇을 연구해보니, perceive, manipulate 하는 기본적인 능력이 전혀 없다.</p> <ul> <li>General Purpose Robot 연구를 위한 필수 요소 <ol> <li> \[\red{\text{Perceive and Manipulate Object}}\] </li> <li>Solve Long-horizon sequential</li> <li>Add Semantic, Common sense</li> </ol> </li> </ul> <p>Today’s topic is the first thing.</p> <ul> <li>교수님의 보통 아이디어 building: 큰 문제 \(\rightarrow\) 작은 문제로 나눔. <ol> <li>Limited action repertorie</li> <li>Representation and perception - How do I represent obejct states?</li> <li>Big data for robotics - How do we efficiently generate one for robots?</li> </ol> </li> </ul> <p>그러나 많은 manipulation 연구가 Pick-n-Place라는 skill에만 국한됨; Prehensile manipulation에 치중되어 있음.</p> <ul> <li>Intuition: Not all objects are graspable.</li> <li>Previous approaches: Physics modeling + Planning으로 해결함. <ul> <li>Limitation: <ul> <li>Estimating the properties from RGB images is extremely difficult.</li> <li>Modeling contact is still an active area of research. They make simplifying assumptions.</li> <li>Planning trajecories take significant amount of time.</li> </ul> </li> </ul> </li> </ul> <h4 id="1-limited-action-repertorie-non-prehensile-tasks">1. Limited action repertorie: Non-Prehensile Tasks</h4> <h5 id="manipulation-system">Manipulation System</h5> <p><a href="https://sites.google.com/view/nonprenehsile-decomposition/home">Pre and Post-Contact Policy Decomposition for Non-Prehensile Manipulation with Zero-Shot Sim-To-Real Transfer</a>: IROS 2023 paper;</p> <ul> <li>너무 크거나 너무 납작한 물체를 밀어서 Pose를 조정함.</li> <li>단차가 있는 벽 위로 물체를 옮겨야 할 때에.</li> </ul> <p>Limitation:</p> <ul> <li>requires a lot of data: isaac sim</li> <li>Exploration is extremly hard for non-prehensile manipulation;</li> <li>기존의 Task definition; The manipulatee is always in close proximity to the manipulator</li> <li>Contact inducing reward를 추가할 수 있음. 다만, 잘 설계해야 함. It may make an ineffective contact.</li> </ul> <p>Approach:</p> <ul> <li>Divided into two stages: 1. Pre-contact phase / 2. Post-contact phase; Tow distint policies.</li> <li>Pre-contact policy Action space; 물체 위의 어느 point에 놓을 것인가, contact point에 대한 RL</li> <li>Post-contact policy Action space; Target end-effector pose (Time-varying Impedance control)</li> </ul> <h5 id="whole-body-manipulation">Whole-Body Manipulation</h5> <p>How to learn Simultaneout balancing and manipulation</p> <ul> <li>Hierarchical policy decomposition + curriculum leraning (이전에는 Series로 수행되었음.)</li> </ul> <p>Lesson learned:</p> <ul> <li>Modularity가 중요하다. 이것이 more efficient learning을 가능하게 함.</li> <li>Manipulator에서는 Action space를 따로 정의하는 것이 Exploration에서 더 효율적이었으며, Debug 과정에서 수월함.</li> </ul> <h4 id="2-representation-and-perception---how-do-i-represent-obejct-states">2. Representation and perception - How do I represent obejct states?</h4> <p>움직이는 motion 자체가 너무 느리다. Hardware 자체적인 성능도 아직은 너무 뒤떨어진다. 훨신 빠르고 Dynamic하게 + Learning purposed에 맞춰서 제작하고자 함.</p> <p>Intuition; How do I represent object states?</p> <ul> <li>Setup: Three cameras.</li> <li> <p>Estimating the 3D Spatial occupancy is important; Encoder of a <code class="language-plaintext highlighter-rouge">Shape completion algorithm</code></p> </li> <li>어떠한 Signal이 [high/Low]-value representation에 영향을 끼치는가? <ul> <li>Contact presence와 Loaction이 매우 중요함.</li> </ul> </li> </ul> <p><strong>CORN: Contact-based Object Representation</strong></p> <ul> <li>Patch Transformer <ul> <li>estimated shape \(\rightarrow\) <strong>RRT + Grasping</strong> (Contact-based)</li> </ul> </li> </ul> <h4 id="3-big-data-for-robotics---how-do-we-efficiently-generate-one-for-robots">3. Big data for robotics - How do we efficiently generate one for robots?</h4> <ul> <li>Big data in simulator: <ul> <li>But, <strong>Collision Detection</strong>이 Non-convex object에 대해서 <strong>too slow</strong> <ul> <li>Contact Detection in the simulator is too slow for non-convex object.</li> </ul> </li> <li><strong>GJK</strong> cannot leverage the parallel compuation.</li> <li>Shape encoder \(\rightarrow\) Collision Predictor; A lot of 3D assets to train this.</li> </ul> </li> <li>Contribution: Local similarity. <ul> <li>Contact이 Local geometric에서는 매우 비슷한 양상을 보임.</li> </ul> </li> </ul> <h4 id="질문">질문:</h4> <p>Q. 흡착형은 어때요? A. 오염이 자주 됨.</p>]]></content><author><name></name></author><category term="seminar"/><category term="Robotics"/><category term="Task and Motion Planning"/><summary type="html"><![CDATA[seminar summary about Beomjoon Kim]]></summary></entry></feed>