<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://joonhyung-lee.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://joonhyung-lee.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-11T05:41:22+00:00</updated><id>https://joonhyung-lee.github.io//feed.xml</id><title type="html">Joel Lee</title><subtitle>Robotics Research Master&apos;s Student </subtitle><entry><title type="html">Real-Time Operating System(RTOS) in Jetson</title><link href="https://joonhyung-lee.github.io//blog/2025/rtos-jetson/" rel="alternate" type="text/html" title="Real-Time Operating System(RTOS) in Jetson"/><published>2025-06-04T00:00:00+00:00</published><updated>2025-06-04T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2025/rtos-jetson</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2025/rtos-jetson/"><![CDATA[<h2 id="rtos란-무엇인가">RTOS란 무엇인가?</h2> <blockquote> <p><strong>RTOS (Real-Time Operating System)</strong>는 특정 작업이 <strong>정해진 시간 안에 반드시 실행되어야 하는</strong> 시스템을 위한 운영체제이다. Jetson 플랫폼에서 RTOS를 적용하는 주된 목적은 <strong>지연 시간을 줄이고</strong>, <strong>결정론적(deterministic)</strong> 처리를 보장하는 데 있다.</p> </blockquote> <h3 id="일반-커널과-rt-커널의-차이">일반 커널과 RT 커널의 차이</h3> <table> <thead> <tr> <th>항목</th> <th>일반 커널</th> <th>RT 커널 (PREEMPT_RT)</th> </tr> </thead> <tbody> <tr> <td>작업 응답 시간</td> <td>Best-effort</td> <td>결정론적 (Deterministic)</td> </tr> <tr> <td>인터럽트 처리</td> <td>대부분 하드 IRQ</td> <td>Thread 기반 softirq</td> </tr> <tr> <td>스케줄링</td> <td>CFS (공정성 중심)</td> <td>FIFO / Round-Robin (우선순위 기반)</td> </tr> <tr> <td>커널 선점성</td> <td>부분적</td> <td>완전 선점 가능</td> </tr> <tr> <td>사용 사례</td> <td>데스크탑, 서버</td> <td>로봇 제어, 산업 제어, 자율주행 등</td> </tr> </tbody> </table> <h2 id="jetson에서-rt-커널-설치-방법">Jetson에서 RT 커널 설치 방법</h2> <p>Jetson에서는 NVIDIA가 제공하는 RT 커널을 <strong>간단히 패키지 설치 방식으로 적용</strong>할 수 있다.</p> <h3 id="1-커널-버전-확인">1. 커널 버전 확인</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">uname</span> <span class="nt">-r</span>
</code></pre></div></div> <ul> <li>일반 커널: <code class="language-plaintext highlighter-rouge">5.15.148-tegra</code></li> <li>RT 커널: <code class="language-plaintext highlighter-rouge">5.15.148-rt-tegra</code></li> </ul> <h3 id="2-저장소-등록">2. 저장소 등록</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>vi /etc/apt/sources.list.d/nvidia-l4t-apt-source.list
</code></pre></div></div> <p>아래의 내용을 추가한다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>deb https://repo.download.nvidia.com/jetson/rt-kernel r36.4 main
</code></pre></div></div> <h3 id="3-rt-커널-설치">3. RT 커널 설치</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>nvidia-l4t-rt-kernel nvidia-l4t-rt-kernel-headers nvidia-l4t-rt-kernel-oot-modules nvidia-l4t-display-rt-kernel
<span class="nb">sudo </span>reboot
</code></pre></div></div> <h3 id="4-부트-커널-전환-설치제거-없이">4. 부트 커널 전환 (설치/제거 없이)</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>vi /boot/extlinux/extlinux.conf
<span class="c"># DEFAULT 값을 변경:</span>
<span class="c"># - 실시간 커널: DEFAULT real-time</span>
<span class="c"># - 일반 커널: DEFAULT primary</span>
<span class="nb">sudo </span>reboot
</code></pre></div></div> <details> <summary>extlinux.conf 파일 내용</summary> ```bash TIMEOUT 30 DEFAULT real-time MENU TITLE L4T boot options LABEL primary MENU LABEL primary kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 mminit_loglevel=4 console=ttyTCU0,115200 firmware_class.path=/etc/firmware fbcon=map:0 nospectre_bhb video=efifb:off console=tty0 # When testing a custom kernel, it is recommended that you create a backup of # the original kernel and add a new entry to this file so that the device can # fallback to the original kernel. To do this: # # 1, Make a backup of the original kernel # sudo cp /boot/Image /boot/Image.backup # # 2, Copy your custom kernel into /boot/Image # # 3, Uncomment below menu setting lines for the original kernel # # 4, Reboot # LABEL backup # MENU LABEL backup kernel # LINUX /boot/Image.backup # INITRD /boot/initrd # APPEND ${cbootargs} LABEL real-time MENU LABEL real-time kernel LINUX /boot/Image.real-time INITRD /boot/initrd APPEND ${cbootargs} root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 mminit_loglevel=4 console=ttyTCU0,115200 firmware_class.path=/etc/firmware fbcon=map:0 nospectre_bhb video=efifb:off console=tty0 ``` </details> <ul> <li>예시) DEFAULT 값을 <code class="language-plaintext highlighter-rouge">real-time</code>으로 설정:</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DEFAULT real-time
</code></pre></div></div> <ul> <li>reboot 후 <code class="language-plaintext highlighter-rouge">uname -r</code>로 확인</li> </ul> <h3 id="5-제거-시">5. 제거 시</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt remove nvidia-l4t-rt-kernel nvidia-l4t-rt-kernel-headers nvidia-l4t-rt-kernel-oot-modules nvidia-l4t-display-rt-kernel
<span class="nb">sudo </span>reboot
</code></pre></div></div> <h2 id="python에서-실시간-우선순위-설정하기">Python에서 실시간 우선순위 설정하기</h2> <p>실시간 커널만으로는 모든 사용자 애플리케이션이 실시간이 되지 않습니다. Python 스크립트도 실시간 우선순위로 실행되어야 합니다.</p> <h3 id="1-python에-cap_sys_nice-권한-부여">1. Python에 <code class="language-plaintext highlighter-rouge">cap_sys_nice</code> 권한 부여</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>setcap <span class="s1">'cap_sys_nice=eip'</span> /usr/bin/python3.8
</code></pre></div></div> <h3 id="2-실시간-스케줄링-코드-예시">2. 실시간 스케줄링 코드 예시</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ctypes</span>

<span class="n">SCHED_FIFO</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">class</span> <span class="nc">SchedParam</span><span class="p">(</span><span class="n">ctypes</span><span class="p">.</span><span class="n">Structure</span><span class="p">):</span>
    <span class="n">_fields_</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">'</span><span class="s">sched_priority</span><span class="sh">'</span><span class="p">,</span> <span class="n">ctypes</span><span class="p">.</span><span class="n">c_int</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">set_realtime_priority</span><span class="p">(</span><span class="n">priority</span><span class="o">=</span><span class="mi">99</span><span class="p">):</span>
    <span class="n">libc</span> <span class="o">=</span> <span class="n">ctypes</span><span class="p">.</span><span class="nc">CDLL</span><span class="p">(</span><span class="sh">'</span><span class="s">libc.so.6</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">param</span> <span class="o">=</span> <span class="nc">SchedParam</span><span class="p">(</span><span class="n">priority</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">libc</span><span class="p">.</span><span class="nf">sched_setscheduler</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">SCHED_FIFO</span><span class="p">,</span> <span class="n">ctypes</span><span class="p">.</span><span class="nf">byref</span><span class="p">(</span><span class="n">param</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">실시간 우선순위 설정 실패</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="3-사용-시">3. 사용 시</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">set_realtime_priority</span><span class="p">()</span>
</code></pre></div></div> <h2 id="rtos를-활용한-ros2-설계-시-고려사항">RTOS를 활용한 ROS2 설계 시 고려사항</h2> <ul> <li><code class="language-plaintext highlighter-rouge">rclcpp::MultiThreadedExecutor</code>를 활용하여 ROS 콜백 스레드를 분리할 것</li> <li>실시간 요구가 있는 노드에는 <code class="language-plaintext highlighter-rouge">ReentrantCallbackGroup</code> 사용</li> <li>동적 메모리 할당, 동기화 객체 사용을 최소화하여 <strong>RT-safe</strong> 코드로 작성</li> <li>노드 간 통신은 DDS QoS 설정 (e.g., <code class="language-plaintext highlighter-rouge">reliable</code>, <code class="language-plaintext highlighter-rouge">deadline</code>)을 통해 시간 보장</li> </ul> <h2 id="결론">결론</h2> <p>RT 커널 설치 이후에도 Python이나 ROS2 환경에서 적절한 스케줄링과 구조 설계가 병행되어야 진정한 RTOS-like 시스템을 구축할 수 있다.</p> <hr/> <h3 id="references">References</h3> <ul> <li><a href="https://docs.nvidia.com/jetson/archives/r36.3/DeveloperGuide/SD/SoftwarePackagesAndTheUpdateMechanism.html#real-time-kernel-using-ota-update">NVIDIA RT Kernel 공식 문서</a></li> <li><a href="https://forums.developer.nvidia.com/t/applying-a-preempt-rt-patch-to-jetpack-4-5-on-jetson-nano/168428/4">Jetson 포럼의 PREEMPT_RT 적용 사례</a></li> </ul> <h2 id="설치방법-raspberry-pi">설치방법 (Raspberry PI)</h2> <ul> <li><a href="https://lemariva.com/blog/2019/09/raspberry-pi-4b-preempt-rt-kernel-419y-performance-test">Raspberry Pi 4B</a></li> <li><a href="https://sudormrf.run/2022/06/18/raspberrypi-realtime-linux/">raspberrypi-realtime-linux</a></li> </ul>]]></content><author><name></name></author><category term="OS"/><category term="RTOS"/><category term="Jetson"/><category term="2025"/><summary type="html"><![CDATA[Detailed explanation of RTOS in Jetson]]></summary></entry><entry><title type="html">How to Set Up a Python Package</title><link href="https://joonhyung-lee.github.io//blog/2024/python-package/" rel="alternate" type="text/html" title="How to Set Up a Python Package"/><published>2024-10-21T00:00:00+00:00</published><updated>2024-10-21T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/python-package</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/python-package/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Creating a Python package is an essential skill for any Python developer. It allows you to organize your code, make it reusable, and share it with others. In this post, we’ll go through the process of setting up a Python package step by step.</p> <h2 id="setting-up-the-package-structure">Setting Up the Package Structure</h2> <ol> <li> <p>Create a new directory for your package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir </span>my_package
<span class="nb">cd </span>my_package
</code></pre></div> </div> </li> <li> <p>Create the following file structure:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>my_package/
├── my_package/
│   ├── __init__.py
│   └── main.py
├── tests/
│   └── test_main.py
├── README.md
├── LICENSE
└── setup.py
</code></pre></div> </div> </li> </ol> <h2 id="writing-setuppy">Writing setup.py</h2> <p>The <code class="language-plaintext highlighter-rouge">setup.py</code> file is crucial for packaging your project. Here’s a basic example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">setuptools</span> <span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>

<span class="nf">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">my_package</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="sh">"</span><span class="s">0.1.0</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">packages</span><span class="o">=</span><span class="nf">find_packages</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">tests</span><span class="sh">"</span><span class="p">]),</span>
    <span class="n">install_requires</span><span class="o">=</span><span class="p">[</span>
        <span class="sh">"</span><span class="s">dependency1</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">dependency2</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">author</span><span class="o">=</span><span class="sh">"</span><span class="s">Your Name</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">author_email</span><span class="o">=</span><span class="sh">"</span><span class="s">your.email@example.com</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A short description of your package</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">long_description</span><span class="o">=</span><span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">README.md</span><span class="sh">"</span><span class="p">).</span><span class="nf">read</span><span class="p">(),</span>
    <span class="n">long_description_content_type</span><span class="o">=</span><span class="sh">"</span><span class="s">text/markdown</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">url</span><span class="o">=</span><span class="sh">"</span><span class="s">https://github.com/yourusername/my_package</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">classifiers</span><span class="o">=</span><span class="p">[</span>
        <span class="sh">"</span><span class="s">Programming Language :: Python :: 3</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">License :: OSI Approved :: MIT License</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Operating System :: OS Independent</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">python_requires</span><span class="o">=</span><span class="sh">"</span><span class="s">&gt;=3.7</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="managing-dependencies">Managing Dependencies</h2> <p>List your package dependencies in the <code class="language-plaintext highlighter-rouge">install_requires</code> parameter of <code class="language-plaintext highlighter-rouge">setup()</code>. For development dependencies, you can create a <code class="language-plaintext highlighter-rouge">requirements.txt</code> file: In <code class="language-plaintext highlighter-rouge">requirements.txt</code>, you can write like below.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Image processing and visualization
opencv-python # or opencv-python-headless
matplotlib

# LangChain and related libraries
langchain
langchain-core
langchain_openai
langchain_anthropic
redis
python-dotenv

# Database
mysql-connector-python

# Additional requirements
numpy
# torch # (Need to check the Jetpack version if on Jetson)
openai
anyio
pydantic
scipy
</code></pre></div></div> <h2 id="version-control-with-git">Version Control with Git</h2> <ol> <li> <p>Initialize a Git repository:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git init
</code></pre></div> </div> </li> <li> <p>Create a <code class="language-plaintext highlighter-rouge">.gitignore</code> file:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>__pycache__/
*.pyc
*.egg-info/
dist/
build/
.pytest_cache/
</code></pre></div> </div> </li> <li> <p>Make your first commit:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add <span class="nb">.</span>
git commit <span class="nt">-m</span> <span class="s2">"Initial commit"</span>
</code></pre></div> </div> </li> </ol> <h2 id="building-and-publishing">Building and Publishing</h2> <ol> <li> <p>Install build tools:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>build twine
</code></pre></div> </div> </li> <li> <p>Build your package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> build
</code></pre></div> </div> </li> <li> <p>Upload to PyPI (ensure you have an account):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>twine upload dist/<span class="k">*</span>
</code></pre></div> </div> </li> </ol> <h2 id="installing-your-package">Installing Your Package</h2> <p>Once published, you can install your package using pip:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>my_package
</code></pre></div></div> <p>For development or to install from GitHub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>git+https://github.com/yourusername/my_package.git
</code></pre></div></div> <h2 id="updating-your-package">Updating Your Package</h2> <ol> <li>Update your code and increment the version number in <code class="language-plaintext highlighter-rouge">setup.py</code>.</li> <li>Rebuild and republish: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> build
twine upload dist/<span class="k">*</span>
</code></pre></div> </div> </li> </ol> <p>To update an installed package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> my_package
</code></pre></div></div> <p>Or for a GitHub-installed package:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="nt">--force-reinstall</span> git+https://github.com/yourusername/my_package.git
</code></pre></div></div> <h2 id="automating-package-publishing-with-github-actions">Automating Package Publishing with GitHub Actions</h2> <p>To automate the process of publishing your package to PyPI whenever you push a new release, you can use GitHub Actions. Here’s how to set it up:</p> <ol> <li>Create a <code class="language-plaintext highlighter-rouge">.github/workflows/publish.yml</code> file in your repository:</li> </ol> <pre><code class="language-yaml:.github/workflows/publish.yml">name: Publish Python Package

on:
  release:
    types: [created]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
    - name: Build and publish
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: $
      run: |
        python -m build
        twine upload dist/*
</code></pre> <p>This workflow will trigger whenever you create a new release on GitHub. It sets up Python, installs the necessary dependencies, builds your package, and uploads it to PyPI using the credentials stored in GitHub Secrets.</p> <ol> <li>Store your PyPI API token as a secret in your GitHub repository: <ul> <li>Go to your repository on GitHub</li> <li>Click on “Settings” &gt; “Secrets” &gt; “New repository secret”</li> <li>Name the secret <code class="language-plaintext highlighter-rouge">PYPI_API_TOKEN</code> and paste your PyPI API token as the value</li> </ul> </li> </ol> <h2 id="managing-pypi-credentials-with-pypirc">Managing PyPI Credentials with .pypirc</h2> <p>For local development and manual uploads, you can use a <code class="language-plaintext highlighter-rouge">.pypirc</code> file to store your PyPI credentials securely. Here’s how to set it up:</p> <ol> <li>Create a <code class="language-plaintext highlighter-rouge">.pypirc</code> file in your home directory:</li> </ol> <pre><code class="language-ini:.pypirc">[distutils]
index-servers =
    pypi
    testpypi

[pypi]
username = __token__
password = your_pypi_api_token

[testpypi]
repository = https://test.pypi.org/legacy/
username = __token__
password = your_testpypi_api_token
</code></pre> <p>Replace <code class="language-plaintext highlighter-rouge">your_pypi_api_token</code> and <code class="language-plaintext highlighter-rouge">your_testpypi_api_token</code> with your actual API tokens for PyPI and TestPyPI respectively.</p> <ol> <li>Set appropriate permissions for the <code class="language-plaintext highlighter-rouge">.pypirc</code> file:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>600 ~/.pypirc
</code></pre></div></div> <p>This ensures that only you can read or write to the file.</p> <p>With this setup, you can easily publish your package using <code class="language-plaintext highlighter-rouge">twine</code> without having to enter your credentials each time. For example:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>twine upload dist/<span class="k">*</span>
</code></pre></div></div> <p>Remember to never commit the <code class="language-plaintext highlighter-rouge">.pypirc</code> file to your version control system, as it contains sensitive information. And finally you can see the installed package like below image.</p> <div align="center"> <img src="/assets/python_package/terminal_result.png" width="95%"/> <p>Installed package</p> </div>]]></content><author><name></name></author><category term="python"/><category term="python"/><category term="setup"/><category term="package"/><category term="2024"/><summary type="html"><![CDATA[A comprehensive guide on creating and managing a Python package]]></summary></entry><entry><title type="html">Setup in Mac OS</title><link href="https://joonhyung-lee.github.io//blog/2024/setup/" rel="alternate" type="text/html" title="Setup in Mac OS"/><published>2024-10-15T00:00:00+00:00</published><updated>2024-10-15T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/setup</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/setup/"><![CDATA[<h2 id="install-homebrew">Install Homebrew</h2> <p>터미널에서 다음 명령어를 실행하세요:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/bin/bash <span class="nt">-c</span> <span class="s2">"</span><span class="si">$(</span>curl <span class="nt">-fsSL</span> https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span class="si">)</span><span class="s2">"</span>
</code></pre></div></div> <p>설치가 완료되면, Homebrew를 PATH에 추가해야 합니다. M1 Mac의 경우 다음 명령어들을 순서대로 실행하세요:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'eval "$(/opt/homebrew/bin/brew shellenv)"'</span> <span class="o">&gt;&gt;</span> ~/.zprofile
<span class="nb">eval</span> <span class="s2">"</span><span class="si">$(</span>/opt/homebrew/bin/brew shellenv<span class="si">)</span><span class="s2">"</span>
</code></pre></div></div> <p>변경사항을 적용하기 위해 터미널을 새로 열거나 다음 명령어로 설정을 다시 불러오세요:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> ~/.zprofile
</code></pre></div></div> <p>Homebrew가 제대로 설치되었는지 확인하려면 다음 명령어를 실행하세요:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nt">--version</span>
</code></pre></div></div> <h2 id="install-pyenv">Install Pyenv</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>pyenv
</code></pre></div></div> <h2 id="install-pyenv-virtualenv">Install Pyenv-virtualenv</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>pyenv-virtualenv
</code></pre></div></div> <p>pyenv activate atlo eval “$(pyenv virtualenv-init -)” pyenv activate atlo</p> <p>pyenv –version pyenv virtualenv –version</p> <p>설정 파일에 pyenv 초기화 코드가 있는지 확인하세요. ~/.zshrc 또는 ~/.bash_profile 파일에 다음 라인들이 있어야 합니다: eval “$(pyenv init -)” eval “$(pyenv virtualenv-init -)” pyenv versions</p> <h2 id="install-jekyll">Install Jekyll</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="nb">install </span>jekyll bundler
<span class="nb">sudo </span>gem <span class="nb">install </span>google-protobuf <span class="nt">-v</span> 3.23.4 <span class="nt">--</span> <span class="nt">--with-cflags</span><span class="o">=</span><span class="s2">"-Wno-error=implicit-function-declaration"</span>
bundle <span class="nb">install
</span>brew <span class="nb">install </span>rbenv
<span class="nb">echo</span> <span class="s1">'eval "$(rbenv init -)"'</span> <span class="o">&gt;&gt;</span> ~/.zshrc
<span class="nb">source</span> ~/.zshrc
<span class="nb">sudo </span>gem <span class="nb">install </span>bundler jekyll

</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rbenv <span class="nb">install </span>3.1.2
rbenv global 3.1.2
</code></pre></div></div> <p>그 이후에 다음의 명령어를 실행하여 로컬에서 서버를 실행할 수 있습니다.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div> <p>최종적으로 다음의 터미널 결과가 나오게 되며, 웹사이트가 구동된다.</p> <div align="center"> <img src="/assets/img/mac/mac_terminal.png" width="95%"/> <p>Mac Terminal</p> </div>]]></content><author><name></name></author><category term="setup"/><category term="setup/mac"/><category term="mac"/><category term="setup"/><category term="2024"/><summary type="html"><![CDATA[Setup in Mac OS]]></summary></entry><entry><title type="html">[paper-review] Text2Reaction : Enabling Reactive Task Planning Using Large Language Models</title><link href="https://joonhyung-lee.github.io//blog/2024/text2react/" rel="alternate" type="text/html" title="[paper-review] Text2Reaction : Enabling Reactive Task Planning Using Large Language Models"/><published>2024-03-24T00:00:00+00:00</published><updated>2024-03-24T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/text2react</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/text2react/"><![CDATA[<blockquote> <p>RA-L, 2024. [<a href="https://ieeexplore.ieee.org/document/10452794">Paper</a>]</p> <p>Zejun Yang , Li Ning, Haitao Wang, Tianyu Jiang, Shaolin Zhang, Shaowei Cui, Hao Jiang, Chunpeng Li, Shuo Wang and Zhaoqi Wang</p> <p>May. 05.</p> </blockquote> <div align="center"> <img src="/assets/img/text2react/overview.png" width="50%"/> <p>Fig. 1: Overview of Text2React.</p> </div> <h4 id="title">Title:</h4> <p>Text2Reaction : Enabling Reactive Task Planning Using Large Language Models (R-AL, 2024)</p> <h4 id="summary">Summary:</h4> <p>They propose Text2Reaction, an LLM-based framework enabling robots to continuously reason and update plans according to the latest environment changes.</p> <h4 id="contribution">Contribution:</h4> <div align="center"> <img src="/assets/img/text2react/flowchart.png" width="50%"/> <p>Fig. 2: Flowchart of Text2React.</p> </div> <div align="center"> <img src="/assets/img/text2react/reasoning-step.png" width="50%"/> <p>Fig. 3: Reasoning Steps of Text2React.</p> </div> <ul> <li>They present the Re-planning Prompt, which informs LLMs the basic principles of re-planning. <ul> <li>It fosters the gradual development of a current plan to a new one in a three-hop reasoning manner: cause analysis, consequence inference, and plan adjustment</li> </ul> </li> <li>OffPlanner: an LLM-based planner that generates initial plans</li> <li>On-Planner: another planner, which updates plans under the guidance of the re-planning prompts</li> </ul> <h4 id="thoughts">Thoughts:</h4> <ul> <li>Re-planning is an important part of the reactive robot. <ul> <li>They showed an LLM-based framework capable of comprehensively analyzing various feedback and continuously re-planning in response to environment changes.</li> </ul> </li> <li>They propose new evaluation metrics for the success rate of task replanning: Executability Rate(ER), Success weighted by Path Length(SPL).</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="paper-review/LLM"/><category term="LLM"/><category term="Object Manipulation"/><category term="Replanning"/><category term="RA-L"/><category term="2024"/><summary type="html"><![CDATA[paper review about Text2React]]></summary></entry><entry><title type="html">[paper-review] CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundational Model</title><link href="https://joonhyung-lee.github.io//blog/2024/copa/" rel="alternate" type="text/html" title="[paper-review] CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundational Model"/><published>2024-03-17T00:00:00+00:00</published><updated>2024-03-17T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/copa</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/copa/"><![CDATA[<blockquote> <p>ArXiv, 2024. [<a href="https://arxiv.org/abs/2403.08248">Paper</a>] [<a href="https://copa-2024.github.io/">Project</a>]</p> <p>Haoxu Huang<sup>2,3,4*</sup>, Fanqi Lin<sup>1,2,4*</sup>, Yingdong Hu<sup>1,2,4</sup>, Shengjie Wang<sup>1,2,4</sup>, Yang Gao<sup>1,2,4</sup> &gt; <sup>1</sup>Institute of Interdisciplinary Information Sciences, Tsinghua University. <sup>2</sup>Shanghai Qi Zhi Institute. <sup>3</sup>Shanghai Jiao Tong University. <sup>4</sup>Shanghai Artificial Intelligence Laboratory. <sup>*</sup> The first two authors contributed equally.</p> <p>Mar. 13.</p> </blockquote> <div align="center"> <img src="/assets/img/copa/copa-overview.png" width="50%"/> <p>Fig. 1: Overview of CoPa.</p> </div> <h4 id="title">Title</h4> <p>CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundational Model (2024, ArXiv)</p> <h4 id="summary">Summary:</h4> <ul> <li>They introduce a framework CoPa, which generates a sequence of 6-DoF end-effector poses for open-world robotic manipulation.They introduce a framework CoPa, which generates a sequence of 6-DoF end-effector poses for open-world robotic manipulation.</li> </ul> <h4 id="contributions">Contributions.</h4> <ul> <li>Task-Oriented Grasping Module <ul> <li>Firstly, they annotate the grasping object leveraging SoM method. (Coarse-Grained Object Grounding)</li> <li>Sequentially crop the image into the region of interest (ROI) of the grasped object. Annotate the grasp contact point in the pixel coordinates of the image. Take a sample grasp pose from GraspNet and match it to the annotated contact point. (Fine-grained part grounding)</li> </ul> </li> <li>Task-Aware Motion Planning Module <ul> <li>This module is used to obtain a series of post-grasp poses. Given the instruction and the current observation, they first employ a grounding module to identify task-relevant parts within the scene.</li> <li>Subsequently, these parts are modeled in 3D, and are then projected and annotated onto the scene image. Following this, VLMs are utilized to generate spatial constraints for these parts. Finally, a solver is applied to calculate the post-grasp poses based on these constraints.</li> </ul> </li> </ul> <h4 id="thoughts">Thoughts.</h4> <ul> <li>They presented their methodology in a very clear way: Combine (I) high-level task planning, which determines what to do next, and (ii) low-level robotic control, focusing on the precise actuation of joints. <ul> <li>Now the GPT-X model can be used in robotic tasks to think like a human.</li> </ul> </li> <li>They demonstrate the seamless integration with ViLa to accomplish long-horizon tasks. <ul> <li>The high-level planner generates a sequence of sub-goals, which are then executed by CoPa.</li> <li>The results show that CoPa can be easily integrated with existing high-level planning algorithms to accomplish complex, long-horizon tasks.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="paper-review/LLM"/><category term="VLMs"/><category term="Object Manipulation"/><category term="ArXiv"/><category term="2024"/><summary type="html"><![CDATA[paper review about CoPa]]></summary></entry><entry><title type="html">[paper-review] MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting</title><link href="https://joonhyung-lee.github.io//blog/2024/moka/" rel="alternate" type="text/html" title="[paper-review] MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting"/><published>2024-03-10T00:00:00+00:00</published><updated>2024-03-10T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/moka</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/moka/"><![CDATA[<blockquote> <p>ArXiv, 2024. [<a href="https://moka-manipulation.github.io/paper.pdf">Paper</a>] [<a href="https://moka-manipulation.github.io/">Project</a>]</p> <p>Kuan Fang<sup>_</sup>, Fangchen Liu<sup>_</sup>, Pieter Abbeel, Sergey Levine</p> <ul> <li>denotes equal contribution, alphabetical order Berkeley AI Research, UC Berkeley</li> </ul> <p>Mar. 05.</p> </blockquote> <div align="center"> <img src="/assets/img/moka/overview.png" width="50%"/> <p>Fig. 1: Overview of MOKA.</p> </div> <h4 id="title">Title</h4> <p>MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting (ArXiv, 2024)</p> <h4 id="summary">Summary</h4> <p>MOKA converts the motion generation problem into a series of visual question-answering problems that the VLM can solve.</p> <h4 id="method">Method</h4> <ul> <li>They introduce a point-based affordance representation that bridges the VLM’s prediction on RGB images and the robot’s motion in the physical world</li> <li>They refer to the physical interaction at each stage as a <em>subtask</em>, which includes interactions with objects in <em>hand</em> (e.g., lifting up an object, opening an drawer), interactions with environmental objects unattached to the robot (e.g., pushing an obstacle, pressing a button), and <em>tool</em> use which involves grasping a tool object to make contact with another object.</li> <li>Decompose the task into a sequence of feasible subtasks based on the free-form language description ( l )</li> <li>Then use VLM (GroundedSAM) to segment objects from the 2D image and overlay them (similar approach to SoM). Additionally, for each of the subtasks, the VLM is asked to provide the summary of the subtask instruction.</li> </ul> <h4 id="thoughts">Thoughts</h4> <ul> <li>This paper’s approach is similar idea with the previous approach of <em>VPI</em> paper. I read this paper with great interest.</li> <li>They have to lift all points from the 2D image into the 6D Cartesian space. So they only consider the cases where the waypoints are at the same height as the target point in most common table manipulation scenarios. <ul> <li>This point would limit the scope of this paper.</li> </ul> </li> <li>Since robust grasping relies on contact physics, they do not rely directly on the predicted grasp pose from the VLM, but use a grasp sampler that is closest to the response grasp pose.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="paper-review/LLM"/><category term="VLMs"/><category term="Set-of-Marks"/><category term="ArXiv"/><category term="2024"/><summary type="html"><![CDATA[paper review about MOKA]]></summary></entry><entry><title type="html">[paper-review] Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</title><link href="https://joonhyung-lee.github.io//blog/2024/rpdiff/" rel="alternate" type="text/html" title="[paper-review] Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement"/><published>2024-03-03T00:00:00+00:00</published><updated>2024-03-03T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/rpdiff</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/rpdiff/"><![CDATA[<blockquote> <p>CoRL, 2023. [<a href="https://arxiv.org/pdf/2307.04751.pdf">Paper</a>] [<a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">Project</a>]</p> <p>Anthony Simeonov, Ankit Goyal<sup>_</sup>, Lucas Manuelli<sup>_</sup>, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal<sup>_</sup><sup>_</sup>, Dieter Fox<sup>_</sup><sup>_</sup> Massachusetts Institute of Technology, NVIDIA Research, Improbable AI Lab</p> <p>Jul. 10.</p> </blockquote> <div align="center"> <img src="/assets/img/rpdiff/overview.png" width="50%"/> <p>Fig. 1: Overview of RPDiff Architecture.</p> </div> <h4 id="title">Title</h4> <p>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement (CoRL, 2023)</p> <h4 id="summary">Summary</h4> <p>Solving placement task in complex environment via diffusion-process.</p> <h4 id="method">Method</h4> <ul> <li>Iteratively de-noise the 6-DoF pose of the object until it satisfies the desired geometric relationship with the scene point cloud.</li> <li>Train a neural network (f_{\theta}) to predict an SE(3) transformation from the combined object-scene point cloud at each time step (t).</li> <li>They also using separate clasifier (h_{\phi}) to avoid “local optimal” solutions by scoring the predicted poses among 0~1.</li> <li>They use transformer for processing point clouds and making pose predictions: 1) identify important geometric parts within the object and the scene, 2) capture relationships that occur between the important parts of the object and the scene.</li> </ul> <h4 id="thoughts">Thoughts</h4> <ul> <li>The paper’s idea is intuitive. They consider (the position and the orientation) of target object and I think it is better than the other real-to-sim approaches.</li> <li>It seems like the paper of “6-dof graspnet” in the context of placement task because this paper consider the value of placement score among 0~1.</li> <li>But the author says that the limitation is “demonstration” data can only be easily obtained via scripted policies in simulation.</li> <li>And I think one more limitation is that they execute the predicted placement in open-loop. Adding the module about reacting or recovering from disturbance would be better.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/manipulation"/><category term="Diffusion Process"/><category term="Manipulation"/><category term="CoRL"/><category term="2023"/><summary type="html"><![CDATA[paper review about RPDiff]]></summary></entry><entry><title type="html">[paper-review] Fast-Replanning Motion Control for Non-Holonomic Vehicles with Aborting A*</title><link href="https://joonhyung-lee.github.io//blog/2024/staa/" rel="alternate" type="text/html" title="[paper-review] Fast-Replanning Motion Control for Non-Holonomic Vehicles with Aborting A*"/><published>2024-02-04T00:00:00+00:00</published><updated>2024-02-04T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/staa</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/staa/"><![CDATA[<blockquote> <p>IROS, 2022. [<a href="https://arxiv.org/pdf/2109.07775.pdf">Paper</a>] [<a href="https://www.youtube.com/watch?v=0Jm7cUAkKmQ">Video</a>]</p> <p>Marcell Missura<sup>1</sup>, Arindam Roychoudhury<sup>1</sup>, Maren Bennewitz<sup>1</sup> &gt; <sup>1</sup>All authors are with the Humanoid Robots Lab, University of Bonn, Germany. Contact: missura@cs.uni-bonn.de</p> <p>Jul. 21.</p> </blockquote> <div align="center"> <img src="/assets/img/staa/pathplanning.png" width="50%"/> <p>Fig. 1: Overview of ShortTerm Aborting A* (STAA*).</p> </div> <div align="center"> <div style="display: flex; justify-content: center; align-items: center;"> <div style="flex: 50%;"> <img src="/assets/img/staa/pathrtr.png" style="width: 100%;"/> <p>Fig. 2: Comparison between RTR and PathRTR.</p> </div> <div style="flex: 50%;"> <img src="/assets/img/staa/simulation-scene.png" style="width: 100%;"/> <p>Fig. 3: Simulation setup.</p> </div> </div> </div> <h3 id="summary">Summary</h3> <ul> <li>They present ShortTerm Aborting A* (STAA*): operates in locally bounded map (seems like dwa algorithm) and avoiding dynamic obstacles using short-term aborting a* algorithm. <ul> <li>They find a global path via Minimal Construct algorithm. Due to the superior performance of the Minimal Construct algorithm, they can afford to recompute the global path in every control cycle; 4.34ms on average.</li> <li>The STAA* motion planner operates in a bounded local map; (8m X 8m square), additionally, they define intermediate global goal pose within the local map.</li> <li>Finally, they locally plan a collision-avoidance path using short-term aborting a*.</li> </ul> </li> </ul> <h3 id="contributions">Contributions</h3> <ul> <li>Local map representation</li> <li>They propose local map representation by inflating convex polygons to avoid planning through too narrow passages the agent would not fit through. <ul> <li>Using <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8594124">Minimal Construct</a> algorithm</li> <li>They set the local goal path as $\tilde{G}$ to plan in the local map.</li> </ul> </li> <li>Short-Term Aborting A*: The centerpiece of this paper is specifically tailored for early abortion. <ul> <li>Like DWA, they sample actions in discrete ranges (in the paper they sample 7x7 actions) based on velocity specs: $(v_{\text{max}}, ~ w_{\text{max}})$</li> </ul> </li> <li>Then they predict the successor state using their dynamic model of the robot. <ul> <li>The radius of the arc is obtained by only using $(v_{\text{max}}, ~ w_{\text{max}})$.</li> </ul> </li> <li>They use SAT algorithm to check collisions. <ul> <li><a href="https://doraeul19.tistory.com/253?category=1066045">Separating Axis Theorem</a>: Collision Detection Using the Separating Axis Theorem</li> </ul> </li> <li>They evaluate heuristic functions leveraging RTR and PathRTR. <ul> <li>rotate-translate-rotate (RTR) time function: estimate the time needed to drive along a path to the intermediate goal and to attain the goal direction.</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Baselines <ul> <li>PD controller, DWA, STAA* (ours)</li> </ul> </li> <li>Evaluation of different heuristic functions <ul> <li>Path RTR (ours)</li> <li>Path Euclidean</li> <li>Dijkstra <ul> <li>when all agents are using STAA* as a planner, almost all collisions can be avoided no matter which heuristic is being used.</li> </ul> </li> </ul> </li> </ul> <h3 id="thoughts">Thoughts</h3> <ul> <li>The demo video shows powerful collision-free navigation and the visualization in the simulation is great.</li> <li>I think the main contribution of this paper is tailored for early abortion. <ul> <li>They also mentioned that one of the parameters is a trade-off between precision and computation time.</li> </ul> </li> <li>And the second-most contribution is a novel time evaluation in goal navigation: Path RTR. They officially released the code, but the implementation is in C++. So I have to study the code and hope to reimplement this algorithm in the mujoco environment.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Navigation"/><category term="Navigation"/><category term="Path Planning"/><category term="IROS"/><category term="2022"/><summary type="html"><![CDATA[paper review about STAA*]]></summary></entry><entry><title type="html">[paper-review] Reactive Base Control for On-The-Move Mobile Manipulation in Dynamic Environments</title><link href="https://joonhyung-lee.github.io//blog/2024/rbc/" rel="alternate" type="text/html" title="[paper-review] Reactive Base Control for On-The-Move Mobile Manipulation in Dynamic Environments"/><published>2024-01-28T00:00:00+00:00</published><updated>2024-01-28T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/rbc</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/rbc/"><![CDATA[<blockquote> <p>RA-L. [<a href="https://arxiv.org/pdf/2309.09393.pdf">Paper</a>] [<a href="https://benburgesslimerick.github.io/MotM-BaseControl/">Project Page</a>]</p> <p>Ben Burgess-Limerick<sup>1, 2</sup>, Jesse Haviland<sup>1, 2</sup>, Chris Lehnert<sup>1</sup>, Peter Corke<sup>1</sup> &gt; <sup>1</sup>Queensland University of Technology Centre for Robotics (QCR), Brisbane, Australia <sup>2</sup> CSIRO Data61, Brisbane, Australia</p> <p>Mar. 03</p> </blockquote> <div align="center"> <img src="/assets/img/rbc/rbc-overview.png" width="50%"/> <p>Fig. 1: Overview of Reactive Base Control.</p> </div> <h3 id="summary">Summary</h3> <ul> <li>This paper presents a reactive-based control method for mobile manipulation in dynamic environments, which significantly reduces the task time and improves the performance of the robot while avoiding static and dynamic obstacles.</li> <li>This approach contrasts with traditional methods where the base and manipulator are controlled separately, often resulting in increased task time due to the sequential completion of navigation and manipulation tasks​.</li> </ul> <div align="center"> <img src="/assets/img/rbc/rbc-staa-orientation.png" width="75%"/> <p>Fig. 2: Inclusion of orientation cost in STAA*.</p> </div> <div align="center"> <img src="/assets/img/rbc/rbc-pathptr.png" width="50%"/> <p>Fig. 3: Bezier path evaluation with PathRTR.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>The paper introduces a reactive base control system for mobile manipulation in dynamic environments, enabling tasks to be performed on-the-move and significantly reducing total task time. This work marks the first real-world demonstration of reactive manipulation with both static and dynamic obstacles, showcasing the method’s practical applicability and robustness. ​ <ul> <li> <ol> <li>They used STAA* algorithm that plangs a collision-free global path by searching through a visibility graph, and then compuites the intersection of the global path and the borader of a local planning region to develop an intermediate goal. The most important addtion to STAA* is the inclusion of an orientation to the goal state. They consider orientation which enables poses to be achieved that smoothly connects the immediate target with the next goal.</li> </ol> </li> <li> <ol> <li>In A* graph search module, they employed PathRTR heuristic. And they combines it with an additional heuristic based on a Bezier curve. It encourages the exploration of states that can be connected to the goal through smooth curves. They said that a value of 25% results in curves that work well in experimental setup.</li> </ol> </li> <li> <ol> <li>Base Placement was optimized by simple cost functions by the weighted sum of two components: <ul> <li> \[C*i = C*{i,C} + 1.05 \cdot C\_{i,N}\] <ul> <li>\(C\_{i,C}\) is the estimated cost from <strong>robot to candidate.</strong></li> <li>\(C\_{i,N}\) is the estimated cost from <strong>the candidate to the next target.</strong></li> </ul> </li> <li>The path cost for each candidate is evaluated using the PathRTR metric.</li> </ul> </li> </ol> </li> <li> <ol> <li>Arm Obstacle Avoidance: They leveraged Quadratic Program to calculate joint veloicited for a given desired end-effector and base velocity. The controller allows for slack in the achieved end-effector velocity.</li> </ol> </li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I was already aware of a previous paper by this author, Manipulation On-The-Move, and it seems to be a follow-up to both this paper and an algorithm called STAA*.</li> <li>I think the contribution of this paper is to add a couple of heuristics to the existing algorithm, and I would have to start with the previous paper; the baseline controller implemented in this paper is all based on the previous paper as a reference. <ul> <li>STAA* / MotM / PathRTR</li> </ul> </li> <li>After reading the paper and watching the demo video, I was quite impressed with the holistic approach to motion planning for mobile-manipulator systems; so accurate and high performance.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Manipulation"/><category term="Mobile Manipulation"/><category term="Manipulation on-the-Move (MotM)"/><category term="Navigation"/><category term="RA-L"/><category term="2024"/><summary type="html"><![CDATA[paper review about Reactive Base Control]]></summary></entry><entry><title type="html">[paper-review] Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction</title><link href="https://joonhyung-lee.github.io//blog/2024/mole/" rel="alternate" type="text/html" title="[paper-review] Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction"/><published>2024-01-20T00:00:00+00:00</published><updated>2024-01-20T00:00:00+00:00</updated><id>https://joonhyung-lee.github.io//blog/2024/mole</id><content type="html" xml:base="https://joonhyung-lee.github.io//blog/2024/mole/"><![CDATA[<blockquote> <p>ICLR. [<a href="https://openreview.net/forum?id=8HCARN2hhw">Paper</a>]</p> <p>Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca Monaci and Christian Wolf <sup>1</sup> &gt; <sup>1</sup>Naver Labs Europe, Meylan, France</p> <p>Sep. 29</p> </blockquote> <div align="center"> <img src="/assets/img/mole/overview.png" width="50%"/> <p>Fig. 1: Overview of MOLE.</p> </div> <h3 id="한-문장-요약">한 문장 요약</h3> <ul> <li>Navigability를 정의하고, 이 latent spatial representation을 학습하자.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li>Instead of learning to reconstruct, they cast the robotic perception task as a navigation task by a blind auxiliary agent generating a learning signal for the main agent.</li> </ul> <div align="center"> <img src="/assets/img/mole/concept.png" width="75%"/> <p>Fig. 2: Concept of MOLE.</p> </div> <div align="center"> <img src="/assets/img/mole/architecture.png" width="100%"/> <p>Fig. 3: Architecture of MOLE.</p> </div> <h3 id="contribution">Contribution</h3> <ul> <li>They propose learning a latent spatial representation (i.e., Navigability). <ul> <li>This approach differs from traditional methods that rely on explicit scene reconstruction. Instead, it relies on a learned latent spatial representation of the environment for navigation.</li> </ul> </li> <li>They define representation \(r_t\) and optimize it based on its amount of information. This representation is refined by a blind auxiliary agent, which operates without direct visual observations, thereby testing and refining its utility for navigation.</li> <li>The author describes the difference between the two methods based on <strong>Behavior Cloning</strong> and <strong>Navigability</strong>. <ul> <li><strong>BC</strong> directly learns the main target policy from expert trajectories, approximating the desired optimal policy. <strong>Navigability</strong>, on the other hand, focuses on learning a representation that optimizes navigational skills (i.e., actions) like detecting navigable space and avoiding obstacles, rather than reconstructing the scene in detail.</li> </ul> </li> </ul> <h3 id="thought">Thought</h3> <ul> <li>I thought that the proposed method seems like a teacher-student network. The main policy (teacher) provides a latent spatial representation (teaching material) that the blind auxiliary agent (student) uses to learn navigational actions.</li> <li>The auxiliary agent’s performance in navigating using this representation gives feedback to improve the main agent’s ability to create effective latent representations. This method offers a more flexible and potentially robust way for robots to navigate diverse environments, especially where creating or relying on detailed maps is impractical or impossible.</li> <li>I think that It’s a notable step forward in the development of autonomous systems that can adapt to a wide range of real-world conditions.</li> </ul>]]></content><author><name></name></author><category term="paper-review"/><category term="paper-review/Navigation"/><category term="Navigation"/><category term="GRU"/><category term="ICLR"/><category term="2024"/><summary type="html"><![CDATA[paper review about Mole]]></summary></entry></feed>